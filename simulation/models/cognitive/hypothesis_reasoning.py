"""
å‡è®¾æ¨ç†ç³»ç»Ÿ
å®ç°å¤šæ­¥é¢„æµ‹æ¨æ¼”å’Œå†³ç­–ä¼˜åŒ–
"""

import json
import copy
from typing import List, Dict, Tuple, Optional, Any
from dataclasses import dataclass
from enum import Enum

from .world_cognition import WorldCognitionDB
from .agent_profile import MultiAgentProfileManager
from .experiment_logger import log_print
from .country_strategy import CountryStrategy

class SatisfactionLevel(Enum):
    """æ»¡æ„åº¦ç­‰çº§"""
    UNACCEPTABLE = 0    # ä¸å¯æ¥å—
    POOR = 1           # è¾ƒå·®
    ACCEPTABLE = 2      # å¯æ¥å—
    GOOD = 3           # è‰¯å¥½
    EXCELLENT = 4      # ä¼˜ç§€


@dataclass
class ReasoningStep:
    """æ¨ç†æ­¥éª¤"""
    step: int                                    # æ­¥éª¤ç¼–å·
    action: str                                  # æ‰§è¡Œçš„è¡Œä¸º
    predicted_world_feedback: Optional[str]      # é¢„æµ‹çš„ä¸–ç•Œåé¦ˆ
    predicted_agent_reactions: Dict[str, str]    # é¢„æµ‹çš„å„Agentååº”
    confidence: float                           # é¢„æµ‹ç½®ä¿¡åº¦
    reasoning_notes: str                        # æ¨ç†æ³¨é‡Š


@dataclass
class ReasoningResult:
    """æ¨ç†ç»“æœ"""
    initial_action: str                         # åˆå§‹é¢„è¡Œä¸º
    reasoning_steps: List[ReasoningStep]        # æ¨ç†æ­¥éª¤é“¾
    final_satisfaction_score: float            # æœ€ç»ˆæ»¡æ„åº¦è¯„åˆ†
    satisfaction_level: SatisfactionLevel      # æ»¡æ„åº¦ç­‰çº§
    reasoning_depth: int                       # æ¨ç†æ·±åº¦
    total_confidence: float                    # æ€»ä½“ç½®ä¿¡åº¦
    

class HypothesisReasoning:
    """å‡è®¾æ¨ç†å¼•æ“"""
    
    def __init__(self, agent_name: str, world_cognition: WorldCognitionDB, 
                 agent_profiles: MultiAgentProfileManager, llm_agent=None, decision_history=None,
                 enable_world_cognition: bool = True,
                 enable_agent_profiles: bool = True):
        self.agent_name = agent_name
        self.world_cognition = world_cognition
        self.agent_profiles = agent_profiles
        self.llm_agent = llm_agent
        self.decision_history = decision_history
        # æ¶ˆèå¼€å…³
        self.use_world_cognition = enable_world_cognition
        self.use_agent_profiles = enable_agent_profiles
        
        # æ¨ç†å‚æ•°
        self.max_reasoning_steps = 1             # æœ€å¤§æ¨ç†æ­¥æ•°
        self.satisfaction_threshold = 0.6        # æ»¡æ„åº¦é˜ˆå€¼
        self.confidence_threshold = 0.5          # ç½®ä¿¡åº¦é˜ˆå€¼
        
        # è¯„åˆ†æƒé‡
        self.world_feedback_weight = 0.4         # ä¸–ç•Œåé¦ˆæƒé‡
        self.agent_reaction_weight = 0.4         # Agentååº”æƒé‡
        self.strategic_value_weight = 0.2        # æˆ˜ç•¥ä»·å€¼æƒé‡
        
        # å¯é€‰ï¼šå›½å®¶ç­–ç•¥ï¼ˆç”±ä¸Šå±‚Agentæ³¨å…¥ï¼‰
        self.country_strategy: CountryStrategy = None
        
        # çµæ´»ç­–ç•¥åŠ¨æ€åˆ‡æ¢æ¨¡å¼ï¼ˆä»…å½“åˆå§‹ä¸ºâ€œçµæ´»â€æ—¶å¯ç”¨ï¼‰
        self._allow_dynamic_strategy_update = False
        self._last_strategy_name = None

    def _is_flexible_strategy(self) -> bool:
        try:
            return bool(self.country_strategy and getattr(self.country_strategy, 'name', '') == 'çµæ´»')
        except Exception:
            return False

    def _init_flex_flag_if_needed(self):
        # åˆæ¬¡çœ‹åˆ°ç­–ç•¥ä¸ºâ€œçµæ´»â€åˆ™æ‰“å¼€åŠ¨æ€åˆ‡æ¢æ¨¡å¼ï¼›è‹¥å·²æ‰“å¼€åˆ™ä¿æŒ
        if self._is_flexible_strategy():
            self._allow_dynamic_strategy_update = True

    def _choose_basic_by_opponent(self) -> Optional[CountryStrategy]:
        """
        æ ¹æ®å¯¹æ‰‹ä¸»å¯¼ç­–ç•¥é€‰æ‹© å¼ºç¡¬/é€€è®©/ä¸€æŠ¥è¿˜ä¸€æŠ¥ã€‚
        - å¼ºç¡¬å€¾å‘: è¿”å› å¼ºç¡¬
        - ç¼“å’Œå€¾å‘: è¿”å› é€€è®©
        - å…¶ä»–/ä¸æ˜ç¡®: è¿”å› ä¸€æŠ¥è¿˜ä¸€æŠ¥
        """
        try:
            # æ±‡æ€»å¯¹æ‰‹ä¸»å¯¼ç­–ç•¥æ–‡æœ¬
            combined = ''
            if self.use_agent_profiles and self.agent_profiles is not None:
                texts = []
                for name, db in self.agent_profiles.profile_dbs.items():
                    ds = db.get_dominant_strategy()
                    if ds:
                        texts.append(ds)
                combined = ' '.join(texts)
            strong_keywords = ["å¼ºç¡¬", "å¨æ…‘", "é€šç‰’", "å¯¹æŠ—", "æ–½å‹", "ç¡¬"]
            soft_keywords = ["å¤–äº¤", "è°ˆåˆ¤", "å’Œå¹³", "æ’¤å›", "åˆä½œ", "è½¯"]
            strong = any(k in combined for k in strong_keywords)
            soft = any(k in combined for k in soft_keywords)
        except Exception:
            strong = False
            soft = False
        # å»¶è¿Ÿå¯¼å…¥å·¥å‚é¿å…å¾ªç¯å¼•ç”¨
        from .country_strategy import (
            make_hardline_strategy,
            make_concession_strategy,
            make_tit_for_tat_strategy,
        )
        if strong and not soft:
            return make_hardline_strategy()
        if soft and not strong:
            return make_concession_strategy()
        # é»˜è®¤ç›¸ç§°æ€§
        return make_tit_for_tat_strategy()

    def _maybe_update_strategy_based_on_opponents(self):
        """
        è‹¥å¯ç”¨çµæ´»æ¨¡å¼ï¼Œåˆ™åœ¨æ¯æ¬¡å†³ç­–å‰æ ¹æ®å¯¹æ‰‹ä¸»å¯¼ç­–ç•¥åˆ‡æ¢ä¸ºä¸‰ç§åŸºæœ¬ç­–ç•¥ä¹‹ä¸€ã€‚
        """
        self._init_flex_flag_if_needed()
        if not self._allow_dynamic_strategy_update:
            return
        new_strategy = self._choose_basic_by_opponent()
        if new_strategy is None:
            return
        prev_name = getattr(self.country_strategy, 'name', 'æœªè®¾ç½®') if self.country_strategy else 'æœªè®¾ç½®'
        # ä»…å½“å½“å‰ä¸æ˜¯ä¸‰åŸºæœ¬æˆ–åå­—ä¸åŒæ‰æ›¿æ¢
        if not self.country_strategy or self.country_strategy.name != new_strategy.name:
            self.country_strategy = new_strategy
            self._log_strategy_change(prev_name, new_strategy.name)

    def set_reasoning_parameters(self, max_steps: int = 3, satisfaction_threshold: float = 0.6,
                               confidence_threshold: float = 0.5):
        """è®¾ç½®æ¨ç†å‚æ•°"""
        self.max_reasoning_steps = max_steps
        self.satisfaction_threshold = satisfaction_threshold
        self.confidence_threshold = confidence_threshold
    
    def hypothesis_reasoning(self, candidate_actions: List[str], 
                           current_context: Dict[str, Any]) -> Tuple[str, ReasoningResult]:
        """
        å‡è®¾æ¨ç†ï¼šå¤šå±‚ç­›é€‰ç­–ç•¥å‡å°‘æˆæœ¬
        è¿”å›ï¼š(best_action, reasoning_result)
        """
        
        # è‹¥ä¸ºçµæ´»ç­–ç•¥ï¼Œå…ˆæ ¹æ®å¯¹æ‰‹ç­–ç•¥æ›´æ–°ä¸ºä¸‰ç§åŸºæœ¬ç­–ç•¥ä¹‹ä¸€
        try:
            self._maybe_update_strategy_based_on_opponents()
        except Exception:
            pass
        
        # æ³¨å…¥ç­–ç•¥å…ƒä¿¡æ¯
        try:
            current_context = current_context.copy()
            current_context['current_strategy'] = {
                'name': self._current_strategy_name(),
                'adapt': bool(getattr(self, 'country_strategy', None) and self.country_strategy.adapt_to_opponent),
                'desc': getattr(self.country_strategy, 'description', None) if getattr(self, 'country_strategy', None) else None
            }
        except Exception:
            pass
        
        # ç¬¬ä¸€å±‚ï¼šè¿‡æ»¤å†å²é‡å¤è¡Œä¸º
        filtered_actions = self._filter_repeated_actions(candidate_actions)
        log_print(f"[å†å²è¿‡æ»¤] {len(candidate_actions)}ä¸ª â†’ è¿‡æ»¤é‡å¤å{len(filtered_actions)}ä¸ª", level="INFO")
        
        # ç¬¬äºŒå±‚ï¼šç­–ç•¥å¼•å¯¼çš„åˆç­› + åŸæœ‰é˜¶æ®µè¿‡æ»¤
        stage_filtered = self._apply_strategy_prescreen(filtered_actions, current_context)
        
        # ç¬¬ä¸‰å±‚ï¼šLLMè¿›ä¸€æ­¥ç­›é€‰
        if len(stage_filtered) > 3:
            top_actions = self._quick_prescreening(stage_filtered, current_context)
            log_print(f"[LLMç­›é€‰] {len(stage_filtered)}ä¸ª â†’ LLMç­›é€‰{len(top_actions)}ä¸ª", level="INFO")
        else:
            top_actions = stage_filtered
            log_print(f"[å€™é€‰è¡Œä¸º] {len(stage_filtered)}ä¸ª â†’ æ— éœ€LLMç­›é€‰", level="INFO")
        
        # ç¬¬å››å±‚ï¼šå¯¹ç­›é€‰å‡ºçš„è¡Œä¸ºè¿›è¡Œè¯¦ç»†æ¨ç†
        reasoning_results = []
        for action in top_actions:
            result = self._multi_step_reasoning(action, current_context)
            reasoning_results.append((action, result))
            log_print(f"[è¡Œä¸ºè¯„ä¼°] {action}: æ»¡æ„åº¦å¾—åˆ†={result.final_satisfaction_score:.2f}, ç½®ä¿¡åº¦={result.total_confidence:.2f}", level="INFO")
        
        # é€‰æ‹©æœ€ä½³è¡Œä¸ºï¼ˆæœ€é«˜æ»¡æ„åº¦è¯„åˆ†ï¼‰
        best_action, best_result = max(reasoning_results, 
                                     key=lambda x: x[1].final_satisfaction_score)
        log_print(f"\n[æœ€ç»ˆé€‰æ‹©] é€‰æ‹©å¾—åˆ†æœ€é«˜çš„è¡Œä¸º: {best_action}", level="INFO")
        log_print(f"[å†³ç­–è¯¦æƒ…] æ»¡æ„åº¦={best_result.final_satisfaction_score:.2f}, ç½®ä¿¡åº¦={best_result.total_confidence:.2f}, æ¨ç†æ·±åº¦={best_result.reasoning_depth}", level="INFO")
        
        return best_action, best_result
    
    def _multi_step_reasoning(self, initial_action: str, 
                            current_context: Dict[str, Any]) -> ReasoningResult:
        """å¯¹å•ä¸ªè¡Œä¸ºè¿›è¡Œå¤šæ­¥æ¨ç†"""
        reasoning_steps = []
        current_action = initial_action
        cumulative_confidence = 1.0
        log_print(f"å¼€å§‹å¯¹{initial_action}è¿›è¡Œå¤šæ­¥æ¨ç†", level="INFO")
        # å¤šæ­¥æ¨æ¼”
        for step in range(self.max_reasoning_steps):
            # é¢„æµ‹ä¸–ç•Œåé¦ˆï¼ˆä½¿ç”¨é™çº§æœºåˆ¶ï¼‰
            if self.use_world_cognition and self.world_cognition is not None:
                predict_feedback, world_experience, world_confidence = \
                    self.world_cognition.predict_feedback_with_fallback(current_action, self.llm_agent)
            else:
                predict_feedback, world_experience, world_confidence = None, None, 0.0
            log_print(f"é¢„æµ‹{current_action}çš„ä¸–ç•Œåé¦ˆ: {predict_feedback}", level="INFO")
            # é¢„æµ‹å„Agentååº”ï¼ˆä½¿ç”¨é™çº§æœºåˆ¶ï¼‰
            agent_reactions = {}
            agent_confidences = []
            
            if self.use_agent_profiles and self.agent_profiles is not None:
                for target_agent in self.agent_profiles.profile_dbs.keys():
                    profile_db = self.agent_profiles.get_profile_db(target_agent)
                    if profile_db is not None:
                        reaction, strategy, experience, confidence = profile_db.predict_reaction_with_fallback(current_action, self.llm_agent)
                        agent_reactions[target_agent] = reaction
                        agent_confidences.append(confidence)
                        log_print(f"é¢„æµ‹{current_action}çš„{target_agent}ååº”: {reaction}", level="INFO")
                    else:
                        log_print(f"æ— æ³•è·å–Agent {target_agent} çš„ä¾§å†™åº“", level="INFO")
            
            # è®¡ç®—æ­¥éª¤ç½®ä¿¡åº¦
            all_confidences = [world_confidence] + agent_confidences
            step_confidence = sum(all_confidences) / len(all_confidences) if all_confidences else 0.0
            cumulative_confidence *= step_confidence
            
            # è®°å½•æ¨ç†æ­¥éª¤
            reasoning_step = ReasoningStep(
                step=step + 1,
                action=current_action,
                predicted_world_feedback=predict_feedback,
                predicted_agent_reactions=agent_reactions,
                confidence=step_confidence,
                reasoning_notes=f"åŸºäºä¸–ç•Œè®¤çŸ¥å’ŒAgentä¾§å†™çš„é¢„æµ‹"
            )
            reasoning_steps.append(reasoning_step)
            
            # å¦‚æœç½®ä¿¡åº¦å¤ªä½ï¼Œæå‰ç»“æŸæ¨ç†
            if step_confidence < self.confidence_threshold:
                break
            
            # ä¸ºä¸‹ä¸€æ­¥æ¨ç†é€‰æ‹©è¡Œä¸ºï¼ˆåŸºäºå½“å‰é¢„æµ‹ç»“æœï¼‰
            if step < self.max_reasoning_steps - 1:
                current_action = self._select_next_action(predict_feedback, agent_reactions, current_context)
                log_print(f"é€‰æ‹©çš„ä¸‹ä¸€æ­¥è¡Œä¸ºä¸º: {current_action}", level="INFO")
                if not current_action:
                    break  # æ— æ³•ç»§ç»­æ¨ç†
        
        # è®¡ç®—æœ€ç»ˆæ»¡æ„åº¦è¯„åˆ†
        final_score = self._calculate_satisfaction_score(reasoning_steps, current_context)
        satisfaction_level = self._get_satisfaction_level(final_score)
        
        return ReasoningResult(
            initial_action=initial_action,
            reasoning_steps=reasoning_steps,
            final_satisfaction_score=final_score,
            satisfaction_level=satisfaction_level,
            reasoning_depth=len(reasoning_steps),
            total_confidence=cumulative_confidence
        )
    
    def _select_next_action(self, world_feedback: Optional[str], 
                          agent_reactions: Dict[str, str], 
                          context: Dict[str, Any]) -> Optional[str]:
        """åŸºäºå½“å‰é¢„æµ‹ç»“æœé€‰æ‹©ä¸‹ä¸€æ­¥è¡Œä¸º"""
        if not self.llm_agent:
            return None
        
        # è·å–ç›¸å…³çš„å†å²ç»éªŒå’Œç­–ç•¥
        world_experiences = []
        agent_strategies = {}
        
        # æ”¶é›†ä¸–ç•Œè®¤çŸ¥ç»éªŒ
        if self.use_world_cognition and self.world_cognition is not None:
            for action in context.get('available_actions', []):
                _, experience, _ = self.world_cognition.predict_feedback(action)
                if experience:
                    world_experiences.append(f"è¡Œä¸º'{action}': {experience}")
        
        # æ”¶é›†Agentç­–ç•¥
        if self.use_agent_profiles and self.agent_profiles is not None:
            for target_agent in agent_reactions.keys():
                profile_db = self.agent_profiles.get_profile_db(target_agent)
                if profile_db is not None:
                    dominant_strategy = profile_db.get_dominant_strategy()
                    if dominant_strategy:
                        agent_strategies[target_agent] = dominant_strategy
        
        # æ„å»ºä¸‹ä¸€æ­¥è¡Œä¸ºé€‰æ‹©çš„æç¤ºè¯
        current_situation = context.get('current_situation', '')
        available_actions = context.get('available_actions', [])
        world_feedback_text = world_feedback or 'æ— é¢„æµ‹'
        reactions_text = ""
        for agent in agent_reactions.keys():
            reactions_text += f"{agent}: {agent_reactions[agent]}\n"
        experiences_text = "\n".join(world_experiences)
        strategies_text = ""
        for agent in agent_strategies.keys():
            strategies_text += f"{agent}: {agent_strategies[agent]}\n"
        if reactions_text=='':
            reactions_text = 'æ— '
        if experiences_text=='':
            experiences_text = 'æ— '
        if strategies_text=='':
            strategies_text = 'æ— '
        decision_count = len(self.decision_history) if self.decision_history else 0

        # å½“å‰ç­–ç•¥ä¿¡æ¯
        strategy_info = context.get('current_strategy', {})
        strategy_line = f"å½“å‰ç­–ç•¥ï¼š{strategy_info.get('name','æœªè®¾ç½®')} (è‡ªé€‚åº”={strategy_info.get('adapt', False)})\næè¿°ï¼š{strategy_info.get('desc','')}"
        
        # ç­–ç•¥æŒ‡å¯¼ï¼ˆæ³¨å…¥å›½å®¶ç­–ç•¥æ–‡æ¡ˆï¼Œç»“åˆå¯¹æ‰‹ä¸»å¯¼ç­–ç•¥ï¼‰
        if self.country_strategy is not None:
            opponent_strategies = {}
            if self.use_agent_profiles and self.agent_profiles is not None:
                for agent in agent_reactions.keys():
                    profile_db = self.agent_profiles.get_profile_db(agent)
                    if profile_db is not None:
                        ds = profile_db.get_dominant_strategy()
                        if ds:
                            opponent_strategies[agent] = ds
            strategic_guidance = self.country_strategy.build_guidance(decision_count, opponent_strategies)
        else:
            # å›é€€åˆ°åŸæœ‰çš„åˆ†é˜¶æ®µæŒ‡å¯¼
            if decision_count <= 2:
                strategic_guidance = """**åˆæœŸè¡ŒåŠ¨æŒ‡å¯¼**ï¼š
- ä¼˜å…ˆé€‰æ‹©èƒ½å»ºç«‹å¨æ…‘å’Œå±•ç¤ºå†³å¿ƒçš„è¡Œä¸º
- æ˜ç¡®åˆ’å®šçº¢çº¿ï¼Œä¸å› â€œç»´æŠ¤å’Œå¹³â€è€Œè¿‡æ—©é€€è®©
- æµ‹è¯•å¯¹æ–¹ååº”ï¼Œä¸ºåç»­è¡ŒåŠ¨æ”¶é›†ä¿¡æ¯
- é¿å…è¿‡åº¦æ¿€åŒ–ï¼Œä½†ç»ä¸èƒ½æ˜¾ç¤ºè½¯å¼±

å†³ç­–é‡ç‚¹ï¼šç¡®ç«‹æˆ˜ç•¥åœ°ä½ > é¿å…å†²çª"""
            elif decision_count <= 5:
                strategic_guidance = """**ä¸­æœŸè¡ŒåŠ¨æŒ‡å¯¼**ï¼š
- åœ¨å·²ç¡®ç«‹åœ°ä½çš„åŸºç¡€ä¸Šï¼Œçµæ´»è°ƒæ•´ç­–ç•¥
- å¯¹å¯¹æ–¹çš„å¼ºç¡¬ç»™äºˆç›¸åº”å›åº”ï¼Œå¯¹ç¼“å’Œç»™äºˆç§¯æä¿¡å·
- è¯„ä¼°å‡çº§æˆæœ¬ï¼Œå¯»æ‰¾æ—¢ç»´æŠ¤åˆ©ç›Šåˆæ§åˆ¶é£é™©çš„é€‰æ‹©
- å¼€å§‹é‡Šæ”¾æœ‰æ¡ä»¶åˆä½œçš„å¯èƒ½æ€§

å†³ç­–é‡ç‚¹ï¼šå¹³è¡¡å¨æ…‘ä¸ç¼“å’Œ"""
            else:
                strategic_guidance = """**åæœŸè¡ŒåŠ¨æŒ‡å¯¼**ï¼š
- ä¸»åŠ¨å¯»æ‰¾åŒæ–¹éƒ½èƒ½æ¥å—çš„è§£å†³æ–¹æ¡ˆ
- åœ¨ä¸æŸå®³æ—¢å¾—åˆ©ç›Šçš„å‰æä¸‹ï¼Œåˆ›é€ ç¼“å’Œæœºä¼š
- é¿å…æ„æ°”ç”¨äº‹ï¼Œä¸“æ³¨äºå®è´¨æ€§åˆ©ç›Š
- ä¸ºå¯¹æ–¹æä¾›ä½“é¢çš„ä¸‹å°é˜¶

å†³ç­–é‡ç‚¹ï¼šå¯»æ±‚å¯æŒç»­è§£å†³æ–¹æ¡ˆ"""
        
        prompt = f"""
        åŸºäºå½“å‰æ¨æ¼”ç»“æœå’Œé˜¶æ®µæ€§æˆ˜ç•¥è€ƒè™‘ï¼Œé€‰æ‹©ä¸‹ä¸€æ­¥æœ€åˆé€‚çš„è¡Œä¸ºï¼š

        å½“å‰æƒ…æ™¯ï¼š{current_situation}
        å½“å‰ç­–ç•¥ï¼š{strategy_line}
        ä¸–ç•Œåé¦ˆï¼š{world_feedback_text}
        å„æ–¹ååº”ï¼š{reactions_text}
        å†å²ç»éªŒï¼š{experiences_text}
        å„æ–¹ä¸»å¯¼ç­–ç•¥ï¼š{strategies_text}
        å¯é€‰è¡Œä¸ºï¼š{available_actions}
        åšå¼ˆé˜¶æ®µï¼šç¬¬{decision_count + 1}è½®å†³ç­–
        
        {strategic_guidance}
        
        ç»¼åˆå†³ç­–åŸåˆ™ï¼š
        1. **æ—¶æœºåˆ¤æ–­**ï¼šå½“å‰æ˜¯æ–½å‹ã€å¹³è¡¡è¿˜æ˜¯ç¼“å’Œçš„æœ€ä½³æ—¶æœº
        2. **æˆæœ¬æ§åˆ¶**ï¼šè¡Œä¸ºä»£ä»·æ˜¯å¦è¶…å‡ºäº†å¯æ‰¿å—èŒƒå›´
        3. **ä¿¡å·ç®¡ç†**ï¼šé€šè¿‡è¡Œä¸ºå‘å„æ–¹ä¼ è¾¾æ¸…æ™°æˆ˜ç•¥æ„å›¾
        4. **ç©ºé—´ä¿ç•™**ï¼šä¸ºæœªæ¥è¡ŒåŠ¨å’Œè°ˆåˆ¤ä¿ç•™å¿…è¦é€‰æ‹©
        5. **åˆ©ç›Šç»´æŠ¤**ï¼šåœ¨ä»»ä½•é˜¶æ®µéƒ½ä¸èƒ½è½»æ˜“æ”¾å¼ƒæ ¸å¿ƒè¯‰æ±‚

        å›ç­”æ ¼å¼ï¼š
        {{{{
            "next_action": "é€‰æ‹©çš„ä¸‹ä¸€æ­¥è¡Œä¸º",
            "reasoning": "é€‰æ‹©ç†ç”±ï¼ˆç»“åˆé˜¶æ®µåˆ†æã€å½“å‰ç­–ç•¥ä¸å¯¹æ‰‹ç­–ç•¥ï¼‰"
        }}}}
        """
        
        try:
            response = self.llm_agent.get_response(prompt)
            if isinstance(response, dict) and 'next_action' in response:
                return response['next_action']
        except Exception as e:
            log_print(f"é€‰æ‹©ä¸‹ä¸€æ­¥è¡Œä¸ºæ—¶å‡ºé”™: {e}", level="ERROR")
        
        return None
    
    def _calculate_satisfaction_score(self, reasoning_steps: List[ReasoningStep], 
                                   context: Dict[str, Any]) -> float:
        """è®¡ç®—æ»¡æ„åº¦è¯„åˆ† - æ­£ç¡®é€»è¾‘ï¼šåªè¯„ä¼°æœ€ç»ˆç»“æœï¼Œä¸è¯„ä¼°ä¸­é—´æ­¥éª¤"""
        if not reasoning_steps:
            return 0.0
        
        # ğŸ¯ å…³é”®ä¼˜åŒ–ï¼šåªè¯„ä¼°æœ€ç»ˆæ­¥éª¤çš„ç»“æœï¼Œä¸è¯„ä¼°ä¸­é—´è¿‡ç¨‹
        final_step = reasoning_steps[-1]
        
        final_score = 0.0
        
        # è¯„ä¼°æœ€ç»ˆçš„ä¸–ç•Œåé¦ˆ
        world_score, agent_score, strategic_score = self._evaluate_world_feedback(
            final_step.action, final_step.predicted_agent_reactions, final_step.predicted_world_feedback, context)
        
        # ç­–ç•¥åŠ æƒä¸å¥–åŠ±
        bonus = 0.0
        if getattr(self, 'country_strategy', None) is not None:
            decision_count = len(self.decision_history) if self.decision_history else 0
            world_score, agent_score, strategic_score, bonus = self.country_strategy.adjust_component_scores(
                final_step.action, decision_count, world_score, agent_score, strategic_score
            )
        
        final_score += world_score * self.world_feedback_weight
        final_score += agent_score * self.agent_reaction_weight
        final_score += strategic_score * self.strategic_value_weight
        final_score += bonus
        
        # ä¸å†ç”¨ç½®ä¿¡åº¦ç›´æ¥å½±å“åˆ†æ•°ï¼Œè€Œæ˜¯ä½œä¸ºå‚è€ƒå€¼
        total_confidence = sum(step.confidence for step in reasoning_steps) / len(reasoning_steps)
        
        # å¦‚æœç½®ä¿¡åº¦è¿‡ä½ï¼ˆ<0.3ï¼‰ï¼Œç»™äºˆè­¦å‘Š
        if total_confidence < 0.3:
            log_print(f"[è­¦å‘Š] è¡Œä¸º'{final_step.action}'çš„ç½®ä¿¡åº¦è¿‡ä½: {total_confidence:.2f}", level="WARNING")
        # æ³¨å…¥ç­–ç•¥æç¤ºåˆ°æ—¥å¿—
        cs = context.get('current_strategy', {})
        log_print(f"ä¸–ç•Œåé¦ˆè¯„åˆ†: {world_score}, Agentååº”è¯„åˆ†: {agent_score}, æˆ˜ç•¥ä»·å€¼è¯„åˆ†: {strategic_score}, ç­–ç•¥={cs.get('name','æœªè®¾ç½®')} è‡ªé€‚åº”={cs.get('adapt', False)}, æœ€ç»ˆè¯„åˆ†: {final_score}", level="INFO")
        return min(1.0, final_score)
    
    def _filter_repeated_actions(self, candidate_actions: List[str]) -> List[str]:
        """
        è¿‡æ»¤æ‰Agentå·²ç»é€‰æ‹©è¿‡çš„è¡Œä¸º
        """
        if self.decision_history is None:
            log_print("å†³ç­–å†å²ä¸ºNoneï¼Œä¸è¿‡æ»¤é‡å¤è¡Œä¸º", level="DEBUG")
            return candidate_actions
            
        if len(self.decision_history) == 0:
            log_print("å†³ç­–å†å²ä¸ºç©ºåˆ—è¡¨ï¼Œä¸è¿‡æ»¤é‡å¤è¡Œä¸º", level="DEBUG")
            return candidate_actions
        
        log_print(f"å†³ç­–å†å²é•¿åº¦: {len(self.decision_history)}", level="DEBUG")
        
        # è·å–å·²é€‰æ‹©è¿‡çš„è¡Œä¸ºåˆ—è¡¨
        chosen_actions = set()
        for i, record in enumerate(self.decision_history):
            if isinstance(record, dict) and 'chosen_action' in record:
                chosen_action = record['chosen_action']
                if chosen_action:  # ç¡®ä¿ä¸æ˜¯Noneæˆ–ç©ºå­—ç¬¦ä¸²
                    chosen_actions.add(chosen_action)
                    log_print(f"æ·»åŠ å·²é€‰æ‹©è¡Œä¸º: {chosen_action}", level="DEBUG")
                    
        log_print(f"å·²é€‰æ‹©è¿‡çš„è¡Œä¸º: {chosen_actions}", level="INFO")
        
        # è¿‡æ»¤é‡å¤è¡Œä¸º
        filtered_actions = [action for action in candidate_actions if action not in chosen_actions]
        
        # å¦‚æœè¿‡æ»¤åæ²¡æœ‰å€™é€‰è¡Œä¸ºï¼Œä¿ç•™åŸå€™é€‰è¡Œä¸ºï¼ˆé¿å…æ— æ³•å†³ç­–çš„æƒ…å†µï¼‰
        if not filtered_actions:
            log_print("è¿‡æ»¤åæ— å¯é€‰è¡Œä¸ºï¼Œä¿ç•™åŸå€™é€‰åˆ—è¡¨", level="WARNING")
            return candidate_actions
        
        log_print(f"è¿‡æ»¤é‡å¤è¡Œä¸º: {len(candidate_actions)}ä¸ª â†’ {len(filtered_actions)}ä¸ª", level="INFO")
        return filtered_actions
    
    def _filter_early_stage_actions(self, candidate_actions: List[str]) -> List[str]:
        """
        è¿‡æ»¤åˆæœŸä¸åˆé€‚çš„è¡Œä¸ºï¼ˆä»…åœ¨ä¸­æœŸåŠä¹‹åå…è®¸è°ˆåˆ¤ä¸å’Œå¹³åè®®ï¼‰
        """
        decision_count = len(self.decision_history) if self.decision_history else 0
        
        # åˆæœŸé˜¶æ®µï¼ˆ1-2è½®ï¼‰è¿‡æ»¤æ‰ç¼“å’Œæ€§è¡Œä¸º
        if decision_count < 3:
            early_avoid_actions = ["å¤–äº¤è°ˆåˆ¤", "å’Œå¹³åè®®"]
            filtered_actions = [action for action in candidate_actions if action not in early_avoid_actions]
            
            if len(filtered_actions) != len(candidate_actions):
                removed_actions = [action for action in candidate_actions if action in early_avoid_actions]
                log_print(f"[åˆæœŸè¿‡æ»¤] ç¬¬{decision_count+1}è½®å†³ç­–ï¼Œè¿‡æ»¤æ‰: {removed_actions}", level="INFO")
                log_print(f"[åˆæœŸè¿‡æ»¤] {len(candidate_actions)}ä¸ª â†’ {len(filtered_actions)}ä¸ª", level="INFO")
            
            # å¦‚æœè¿‡æ»¤åæ²¡æœ‰è¡Œä¸ºï¼Œä¿ç•™åŸåˆ—è¡¨é¿å…æ— æ³•å†³ç­–
            if not filtered_actions:
                log_print("[åˆæœŸè¿‡æ»¤] è¿‡æ»¤åæ— å¯é€‰è¡Œä¸ºï¼Œä¿ç•™åŸå€™é€‰åˆ—è¡¨", level="WARNING")
                return candidate_actions
            
            return filtered_actions
        else:
            log_print(f"[åˆæœŸè¿‡æ»¤] ç¬¬{decision_count+1}è½®å†³ç­–ï¼Œå·²è¿›å…¥ä¸­æœŸï¼Œä¸è¿‡æ»¤è¡Œä¸º", level="DEBUG")
            return candidate_actions
    
    def _current_strategy_name(self) -> str:
        try:
            if getattr(self, 'country_strategy', None):
                return self.country_strategy.name
        except Exception:
            pass
        return 'æœªè®¾ç½®'

    def _log_strategy_change(self, previous: str, current: str):
        if previous != current:
            log_print(f"[ç­–ç•¥æ›´æ–°] {self.agent_name}: {previous} -> {current}", level="INFO")

    def _apply_strategy_prescreen(self, candidate_actions: List[str], context: Dict[str, Any]) -> List[str]:
        """
        åŸºäºå›½å®¶ç­–ç•¥å¯¹å€™é€‰è¡Œä¸ºè¿›è¡Œåå¥½åŠ æƒä¸ä¸é¼“åŠ±è¿‡æ»¤ï¼Œéšååº”ç”¨åŸæœ‰é˜¶æ®µè¿‡æ»¤ã€‚
        å¯¹äºä¸€æŠ¥è¿˜ä¸€æŠ¥/çµæ´»ç­–ç•¥ï¼Œä¼šæ ¹æ®å¯¹æ‰‹ä¸»å¯¼ç­–ç•¥è¿›ä¸€æ­¥è°ƒæ•´åå¥½ã€‚
        """
        # åŸå§‹é˜¶æ®µè¿‡æ»¤ä½œä¸ºå…œåº•
        base_filtered = self._filter_early_stage_actions(candidate_actions)
        if not self.country_strategy:
            return base_filtered

        # è®°å½•ç­–ç•¥ï¼ˆè‹¥å˜æ›´ï¼‰
        prev = getattr(self, '_last_strategy_name', None)
        cur = self._current_strategy_name()
        self._log_strategy_change(prev or cur, cur)
        self._last_strategy_name = cur

        decision_count = len(self.decision_history) if self.decision_history else 0

        # è·å–å¯¹æ‰‹ä¸»å¯¼ç­–ç•¥ï¼ˆç”¨äºè‡ªé€‚åº”ï¼‰
        opponent_strategies = {}
        if self.country_strategy.adapt_to_opponent and self.use_agent_profiles and self.agent_profiles is not None:
            for name, db in self.agent_profiles.profile_dbs.items():
                ds = db.get_dominant_strategy()
                if ds:
                    opponent_strategies[name] = ds

        # è®¡ç®—æ¯ä¸ªè¡Œä¸ºçš„ç­–ç•¥åå¥½åˆ†
        scored = []
        for act in base_filtered:
            pref = self.country_strategy.action_preference_score(act, decision_count)

            # è‡ªé€‚åº”ï¼šå¦‚æœå¯¹æ‰‹ä¸»å¯¼ç­–ç•¥åå¼ºç¡¬ï¼Œç•¥å¾®æå‡å¨æ…‘ç±»è¡Œä¸ºï¼›åç¼“å’Œï¼Œæå‡å¤–äº¤ç±»
            if opponent_strategies:
                combined = " ".join(opponent_strategies.values())
                strong_keywords = ["å¼ºç¡¬", "å¨æ…‘", "é€šç‰’", "å¯¹æŠ—", "æ–½å‹"]
                soft_keywords = ["å¤–äº¤", "è°ˆåˆ¤", "å’Œå¹³", "æ’¤å›", "åˆä½œ"]
                strong_tendency = any(k in combined for k in strong_keywords)
                soft_tendency = any(k in combined for k in soft_keywords)

                if strong_tendency and act in ["å†›äº‹æ¼”ä¹ ", "åŒºåŸŸå°é”", "æ­¦å™¨éƒ¨ç½²", "ç»æµåˆ¶è£", "æœ€åé€šç‰’"]:
                    pref += 0.02
                if soft_tendency and act in ["å¤–äº¤è°ˆåˆ¤", "å’Œå¹³åè®®", "æ’¤å›è¡ŒåŠ¨", "å…¬å¼€å£°æ˜"]:
                    pref += 0.02

            scored.append((act, max(0.0, min(1.0, pref))))

        # å¦‚æœå…¨éƒ¨ç›¸åŒåˆ†ï¼Œä¸æ”¹é¡ºåºï¼›å¦åˆ™æŒ‰åˆ†æ•°æ’åºï¼Œå–å‰Nï¼ˆè‡³å°‘3ä¸ªï¼Œè‡³å¤š5ä¸ªï¼‰
        if scored:
            max_s = max(s for _, s in scored)
            min_s = min(s for _, s in scored)
            if max_s - min_s > 1e-6:
                scored.sort(key=lambda x: x[1], reverse=True)
                top_k = max(3, min(5, len(scored)))
                return [a for a, _ in scored[:top_k]]
        return base_filtered

    def _quick_prescreening(self, candidate_actions: List[str], context: Dict[str, Any]) -> List[str]:
        """
        å¿«é€Ÿé¢„ç­›é€‰ï¼šç”¨ä¸€æ¬¡LLMè°ƒç”¨ç­›é€‰å‡ºæœ€æœ‰æ½œåŠ›çš„å€™é€‰è¡Œä¸º
        å¤§å¹…å‡å°‘åç»­è¯¦ç»†æ¨ç†çš„æ•°é‡
        """
        if not self.llm_agent or len(candidate_actions) <= 3:
            return candidate_actions
        log_print(f"å¼€å§‹LLMå¿«é€Ÿé¢„ç­›é€‰: {candidate_actions}", level="DEBUG")
        actions_text = ", ".join(f'"{action}"' for action in candidate_actions)
        # å½“å‰ç­–ç•¥ä¿¡æ¯
        strategy_info = context.get('current_strategy', {})
        strategy_line = f"å½“å‰ç­–ç•¥ï¼š{strategy_info.get('name','æœªè®¾ç½®')} (è‡ªé€‚åº”={strategy_info.get('adapt', False)})\næè¿°ï¼š{strategy_info.get('desc','')}"
        
        # è·å–å†å²å†³ç­–æ•°é‡æ¥åˆ¤æ–­åšå¼ˆé˜¶æ®µ
        decision_count = len(self.decision_history) if self.decision_history else 0
        
        # åˆ¤æ–­å½“å‰æ‰€å¤„é˜¶æ®µ
        if decision_count <= 2:
            stage = "åˆæœŸ"
            stage_guidance = """**åˆæœŸç­–ç•¥é‡ç‚¹**ï¼š
- æ˜ç¡®è¡¨è¾¾ç«‹åœºå’Œæ ¸å¿ƒè¯‰æ±‚ï¼Œä¸å¯è¿‡æ—©å¦¥å
- å»ºç«‹å¨æ…‘åŠ›å’Œè°ˆåˆ¤ç­¹ç ï¼Œå±•ç¤ºå†³å¿ƒ
- è¯•æ¢å¯¹æ–¹åº•çº¿ï¼Œä½†é¿å…ä¸å¯é€†çš„æç«¯è¡Œä¸º
- åˆ‡å‹¿å› ä¸ºè¿½æ±‚"ç¼“å’Œ"è€Œæ”¾å¼ƒåˆç†åˆ©ç›Šè¯‰æ±‚
- è½¯å¼±çš„è¡¨ç°åªä¼šæ‹›è‡´æ›´å¤§å‹åŠ›"""
        elif decision_count <= 5:
            stage = "ä¸­æœŸ"
            stage_guidance = """**ä¸­æœŸç­–ç•¥é‡ç‚¹**ï¼š
- åœ¨å·²å»ºç«‹å¨æ…‘çš„åŸºç¡€ä¸Šï¼Œè¯„ä¼°å‡çº§æˆæœ¬
- é€‚åº¦å›åº”å¯¹æ–¹è¡ŒåŠ¨ï¼Œä¿æŒå¹³è¡¡
- å¯»æ‰¾åœ¨ä¸æŸå®³æ ¸å¿ƒåˆ©ç›Šå‰æä¸‹çš„ç¼“å’Œæœºä¼š
- å‘å¯¹æ–¹ä¼ è¾¾æ—¢æœ‰å®åŠ›åˆæœ‰åˆä½œæ„æ„¿çš„ä¿¡å·"""
        else:
            stage = "åæœŸ"
            stage_guidance = """**åæœŸç­–ç•¥é‡ç‚¹**ï¼š
- åŒæ–¹å®åŠ›å·²ç»å±•ç¤ºï¼Œå¯»æ±‚ä½“é¢çš„è§£å†³æ–¹æ¡ˆ
- ä¸»åŠ¨åˆ›é€ ç¼“å’Œæœºä¼šï¼Œä½†ä¸æŸå®³å·²è·å¾—çš„åœ°ä½
- è€ƒè™‘é€šè¿‡è°ˆåˆ¤å®ç°åŒæ–¹éƒ½èƒ½æ¥å—çš„ç»“æœ
- é¿å…ä¸ºé¢å­é—®é¢˜ç»§ç»­æ— æ„ä¹‰çš„å¯¹æŠ—"""
        
        prompt = f"""
        å½“å‰éœ€è¦ä»å¤šä¸ªå€™é€‰è¡Œä¸ºä¸­å¿«é€Ÿç­›é€‰å‡ºæœ€æœ‰æ½œåŠ›çš„è¡Œä¸ºè¿›è¡Œè¯¦ç»†åˆ†æã€‚
        
        å€™é€‰è¡Œä¸º: {actions_text}
        å½“å‰æƒ…å†µ: {context.get('current_situation', '')}
        å½“å‰ç­–ç•¥ï¼š{strategy_line}
        ç›®æ ‡: {context.get('objectives', '')}
        åšå¼ˆé˜¶æ®µ: {stage} (å·²è¿›è¡Œ{decision_count}è½®å†³ç­–)
        
        {stage_guidance}
        
        é€šç”¨è¯„ä¼°åŸåˆ™ï¼š
        1. **ç«‹åœºåšå®šæ€§**ï¼šæ˜¯å¦ç»´æŠ¤äº†æ ¸å¿ƒåˆ©ç›Šè¯‰æ±‚
        2. **ç­–ç•¥åˆç†æ€§**ï¼šæ˜¯å¦ç¬¦åˆå½“å‰é˜¶æ®µçš„æˆ˜ç•¥é‡ç‚¹
        3. **æˆæœ¬æ•ˆç›Š**ï¼šè€ƒè™‘è¡Œä¸ºçš„ä»£ä»·å’Œå¯èƒ½æ”¶ç›Š
        4. **ä¿¡å·ä¼ è¾¾**ï¼šå‘å¯¹æ–¹ä¼ è¾¾ä»€ä¹ˆæˆ˜ç•¥æ„å›¾
        5. **åç»­ç©ºé—´**ï¼šä¸ºä¸‹ä¸€é˜¶æ®µä¿ç•™å“ªäº›é€‰æ‹©

        ç‰¹åˆ«æé†’ï¼š
        - åˆæœŸä¸è¦å› å®³æ€•å†²çªè€Œè¿‡æ—©å¦¥å
        - ä¸­æœŸè¦åœ¨å¨æ…‘å’Œç¼“å’Œé—´å¯»æ‰¾å¹³è¡¡
        - åæœŸè¦ä¸ºåŒæ–¹æä¾›ä½“é¢çš„ä¸‹å°é˜¶
        
        è¿”å›æ ¼å¼ (ä¸¥æ ¼æŒ‰JSONæ ¼å¼):
        {{{{
            "selected_actions": ["è¡Œä¸º1", "è¡Œä¸º2", "è¡Œä¸º3"],
            "reasoning": "ç­›é€‰ç†ç”±ï¼ˆç»“åˆé˜¶æ®µç­–ç•¥åˆ†æï¼‰"
        }}}}
        """
        
        try:
            response = self.llm_agent.get_response(prompt)
            if isinstance(response, dict) and 'selected_actions' in response:
                selected = response['selected_actions']
                # ç¡®ä¿è¿”å›çš„è¡Œä¸ºéƒ½åœ¨åŸå€™é€‰åˆ—è¡¨ä¸­
                valid_selected = [action for action in selected if action in candidate_actions]
                return valid_selected[:3] if valid_selected else candidate_actions[:3]
        except Exception as e:
            log_print(f"é¢„ç­›é€‰å¤±è´¥ï¼Œä½¿ç”¨å‰3ä¸ªè¡Œä¸º: {e}", level="ERROR")
        
        # é™çº§ç­–ç•¥ï¼šç›´æ¥è¿”å›å‰3ä¸ª
        return candidate_actions[:3]
       
    def _evaluate_world_feedback(self, action: str, agent_reactions: Dict[str, str], feedback: str, context: Dict[str, Any]) -> Tuple[float, float, float]:
        """è¯„ä¼°ä¸–ç•Œåé¦ˆçš„å¥½åï¼ˆ0-1åˆ†ï¼‰"""
        
        reactions_text = ""
        for agent in agent_reactions.keys():
            reactions_text += f"{agent}: {agent_reactions[agent]}\n"
        
        objectives = context.get('objectives', '')
        current_situation = context.get('current_situation', '')
        
        decision_count = len(self.decision_history) if self.decision_history else 0
        
        # æ ¹æ®é˜¶æ®µè°ƒæ•´è¯„ä¼°é‡ç‚¹
        if decision_count <= 2:
            evaluation_focus = """**åˆæœŸè¯„ä¼°é‡ç‚¹**ï¼š
- æ˜¯å¦æœ‰æ•ˆå»ºç«‹äº†å¨æ…‘åŠ›å’Œè°ˆåˆ¤åœ°ä½
- æ˜¯å¦æ¸…æ™°ä¼ è¾¾äº†æ ¸å¿ƒåˆ©ç›Šåº•çº¿
- æ˜¯å¦é¿å…äº†è¢«è§†ä¸ºè½¯å¼±çš„é£é™©
- æ˜¯å¦ä¸ºåç»­å‡çº§ä¿ç•™äº†ç©ºé—´"""
        elif decision_count <= 5:
            evaluation_focus = """**ä¸­æœŸè¯„ä¼°é‡ç‚¹**ï¼š
- æ˜¯å¦ç»´æŒäº†å·²å»ºç«‹çš„å¨æ…‘å¹³è¡¡
- æ˜¯å¦é€‚å½“å›åº”äº†å¯¹æ–¹çš„è¡ŒåŠ¨
- æ˜¯å¦åˆ›é€ äº†ç¼“å’Œçš„å¯èƒ½æ€§
- å‡çº§æˆæœ¬æ˜¯å¦ä»ç„¶å¯æ§"""
        else:
            evaluation_focus = """**åæœŸè¯„ä¼°é‡ç‚¹**ï¼š
- æ˜¯å¦æœ‰åŠ©äºå¯»æ‰¾è§£å†³æ–¹æ¡ˆ
- æ˜¯å¦ä¿æŒäº†å·²è·å¾—çš„æˆ˜ç•¥åœ°ä½
- æ˜¯å¦ä¸ºåŒæ–¹æä¾›äº†ä½“é¢çš„å‡ºè·¯
- æ˜¯å¦é¿å…äº†æ— æ„ä¹‰çš„æ„æ°”ä¹‹äº‰"""
        
        prompt = f"""
        è¯„ä¼°ä»¥ä¸‹è¡Œä¸º-åé¦ˆå¯¹æˆ‘æ–¹çš„æœ‰åˆ©ç¨‹åº¦ï¼Œéœ€è¦ç»“åˆåšå¼ˆé˜¶æ®µè¿›è¡Œç»¼åˆåˆ†æï¼š

        æˆ‘æ–¹è¡Œä¸ºï¼š{action}
        ä¸–ç•Œåé¦ˆï¼š{feedback}
        å„æ–¹ååº”ï¼š
        {reactions_text}
        å½“å‰ç›®æ ‡ï¼š{objectives}
        å½“å‰æ€åŠ¿ï¼š{current_situation}
        åšå¼ˆé˜¶æ®µï¼šç¬¬{decision_count + 1}è½®å†³ç­–
        
        {evaluation_focus}
        
        è¯„ä¼°ç»´åº¦ï¼š
        
        **ä¸–ç•Œåé¦ˆè¯„åˆ†** - ç»¼åˆè€ƒè™‘ï¼š
        - æ˜¯å¦æœ‰æ•ˆæ¨è¿›äº†é˜¶æ®µæ€§æˆ˜ç•¥ç›®æ ‡
        - å›½é™…ç¯å¢ƒå¯¹æˆ‘æ–¹è¡Œä¸ºçš„æ¥å—ç¨‹åº¦
        - æ˜¯å¦ç»´æŠ¤äº†é“å¾·åˆ¶é«˜ç‚¹

        **å„æ–¹ååº”è¯„åˆ†** - ç»¼åˆè€ƒè™‘ï¼š
        - å¯¹æ–¹ååº”æ˜¯å¦ç¬¦åˆé¢„æœŸå’Œå¯æ§
        - æ˜¯å¦ä¿ƒè¿›äº†æˆ˜ç•¥å¹³è¡¡è€Œéå¤±æ§å‡çº§
        - ç›Ÿå‹å’Œç¬¬ä¸‰æ–¹çš„æ€åº¦å˜åŒ–

        **æˆ˜ç•¥ä»·å€¼è¯„åˆ†** - ç»¼åˆè€ƒè™‘ï¼š
        - å¯¹é•¿æœŸç›®æ ‡å®ç°çš„è´¡çŒ®åº¦
        - æˆæœ¬æ”¶ç›Šæ¯”çš„åˆç†æ€§
        - å¯¹åç»­è¡ŒåŠ¨ç©ºé—´çš„å½±å“

        è¯„åˆ†åŸåˆ™ï¼š
        - åˆæœŸè¿‡åº¦å¦¥åæ¯”é€‚åº¦å¼ºç¡¬æ›´å±é™©
        - ä¸­æœŸè¦å¹³è¡¡å¨æ…‘ä¸ç¼“å’Œçš„éœ€è¦
        - åæœŸè¦è€ƒè™‘å¯æŒç»­è§£å†³æ–¹æ¡ˆçš„ä»·å€¼

        å›ç­”æ ¼å¼ï¼š
        {{{{
            "world_feedback_score": 0.0-1.0ä¹‹é—´çš„æ•°å€¼,
            "agent_reactions_score": 0.0-1.0ä¹‹é—´çš„æ•°å€¼,
            "strategic_value_score": 0.0-1.0ä¹‹é—´çš„æ•°å€¼,
            "reasoning": "è¯„åˆ†ç†ç”±ï¼ˆç»“åˆé˜¶æ®µæ€§åˆ†æï¼‰"
        }}}}
        """
        
        try:
            response = self.llm_agent.get_response(prompt)
            if isinstance(response, dict) and 'world_feedback_score' in response and 'agent_reactions_score' in response and 'strategic_value_score' in response:
                return float(response['world_feedback_score']), float(response['agent_reactions_score']), float(response['strategic_value_score'])
            else:
                return 0.5, 0.5, 0.5
        except Exception as e:
            log_print(f"è¯„ä¼°ä¸–ç•Œåé¦ˆæ—¶å‡ºé”™: {e}", level="ERROR")
        
        return 0.5, 0.5, 0.5
    
    def _get_satisfaction_level(self, score: float) -> SatisfactionLevel:
        """æ ¹æ®è¯„åˆ†è·å–æ»¡æ„åº¦ç­‰çº§"""
        if score >= 0.8:
            return SatisfactionLevel.EXCELLENT
        elif score >= 0.6:
            return SatisfactionLevel.GOOD
        elif score >= 0.4:
            return SatisfactionLevel.ACCEPTABLE
        elif score >= 0.2:
            return SatisfactionLevel.POOR
        else:
            return SatisfactionLevel.UNACCEPTABLE
    
    def should_accept_action(self, reasoning_result: ReasoningResult) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ¥å—è¯¥è¡Œä¸º"""
        return (reasoning_result.final_satisfaction_score >= self.satisfaction_threshold and
                reasoning_result.satisfaction_level != SatisfactionLevel.UNACCEPTABLE)
    

    

    

    
    def get_reasoning_summary(self, reasoning_result: ReasoningResult) -> str:
        """è·å–æ¨ç†ç»“æœæ‘˜è¦"""
        summary = f"""
æ¨ç†æ‘˜è¦ï¼š
- åˆå§‹è¡Œä¸ºï¼š{reasoning_result.initial_action}
- æ¨ç†æ·±åº¦ï¼š{reasoning_result.reasoning_depth}æ­¥
- æ»¡æ„åº¦è¯„åˆ†ï¼š{reasoning_result.final_satisfaction_score:.2f}
- æ»¡æ„åº¦ç­‰çº§ï¼š{reasoning_result.satisfaction_level.name}
- æ€»ä½“ç½®ä¿¡åº¦ï¼š{reasoning_result.total_confidence:.2f}

æ¨ç†è¿‡ç¨‹ï¼š
"""
        for step in reasoning_result.reasoning_steps:
            summary += f"\nç¬¬{step.step}æ­¥ï¼š{step.action}"
            summary += f"\n  - ä¸–ç•Œåé¦ˆï¼š{step.predicted_world_feedback or 'æ— '}"
            summary += f"\n  - Agentååº”ï¼š{step.predicted_agent_reactions}"
            summary += f"\n  - ç½®ä¿¡åº¦ï¼š{step.confidence:.2f}"
        
        return summary