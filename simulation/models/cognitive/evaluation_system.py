"""
å¤šAgentåšå¼ˆåœºæ™¯æ¨¡å‹æ•ˆæœè¯„æµ‹ç³»ç»Ÿ
å®ç°å››ä¸ªè¯„ä»·æŒ‡æ ‡ï¼šEAã€ASã€SRã€OM
"""

import json
import numpy as np
from typing import List, Dict, Any, Tuple, Optional
from datetime import datetime
from dataclasses import dataclass
from scipy.stats import kendalltau
import re
from pathlib import Path
import os
from openai import OpenAI
from sklearn.metrics.pairwise import cosine_similarity

@dataclass
class GameRoundData:
    """æ¸¸æˆè½®æ¬¡æ•°æ®ç»“æ„"""
    round: int
    timestamp: str
    actor: str
    declaration: str
    action: str
    world_feedback: str

@dataclass
class EvaluationResult:
    """è¯„æµ‹ç»“æœæ•°æ®ç»“æ„"""
    ea_score: float  # å†å²äº‹ä»¶å¯¹é½åº¦
    as_score: float  # è¡ŒåŠ¨å†…å®¹ç›¸ä¼¼åº¦
    sr_score: float  # æˆ˜ç•¥åˆç†æ€§
    om_score: float  # ç»“æœä¸€è‡´æ€§
    final_score: float  # æœ€ç»ˆå¾—åˆ†
    detailed_metrics: Dict[str, Any]  # è¯¦ç»†æŒ‡æ ‡

class EventAlignmentEvaluator:
    """å†å²äº‹ä»¶å¯¹é½åº¦è¯„ä¼°å™¨"""
    
    def __init__(self):
        # å®šä¹‰å…³é”®å†å²äº‹ä»¶ç±»å‹ï¼ˆåŸºäºçœŸå®å¤å·´å¯¼å¼¹å±æœºäº‹ä»¶åºåˆ—ï¼‰
        self.historical_event_types = [
            "æ­¦å™¨éƒ¨ç½²", "åŒºåŸŸå°é”", "å…¬å¼€å£°æ˜", "å¤–äº¤è°ˆåˆ¤", 
            "æœ€åé€šç‰’", "æ’¤å›è¡ŒåŠ¨", "å’Œå¹³åè®®", "å†›äº‹æ¼”ä¹ ",
            "ç»æµåˆ¶è£", "æƒ…æŠ¥ä¾¦å¯Ÿ", "å®£æˆ˜", "æ ¸æ‰“å‡»"
        ]
        
        # æ ‡å‡†å†å²æ—¶é—´çº¿ï¼ˆåŸºäºçœŸå®å†å²æ•°æ®è°ƒæ•´æ—¶é—´çª—å£ï¼‰
        self.historical_timeline = [
            {"type": "æ­¦å™¨éƒ¨ç½²", "window": (1, 2)},      # round 1
            {"type": "åŒºåŸŸå°é”", "window": (2, 4)},      # round 2 
            {"type": "å…¬å¼€å£°æ˜", "window": (3, 5)},      # round 3
            {"type": "å¤–äº¤è°ˆåˆ¤", "window": (4, 8)},      # round 4-5
            {"type": "æœ€åé€šç‰’", "window": (6, 8)},      # round 6
            {"type": "æ’¤å›è¡ŒåŠ¨", "window": (7, 10)},     # round 7-8
            {"type": "å’Œå¹³åè®®", "window": (9, 12)}      # round 9-10
        ]
        
    def _classify_action_type(self, action: str) -> Optional[str]:
        """å°†è¡ŒåŠ¨åˆ†ç±»ä¸ºå†å²äº‹ä»¶ç±»å‹"""
        action_clean = action.strip()
        
        # ç›´æ¥æ˜ å°„æ ‡å‡†è¡ŒåŠ¨åç§°
        if action_clean in self.historical_event_types:
            return action_clean
        
        # å¦‚æœä¸æ˜¯æ ‡å‡†åç§°ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…
        action_lower = action_clean.lower()
        classification_map = {
            "æ­¦å™¨éƒ¨ç½²": ["éƒ¨ç½²", "å¯¼å¼¹", "æ ¸æ­¦å™¨", "æ­¦å™¨ç³»ç»Ÿ"],
            "åŒºåŸŸå°é”": ["å°é”", "ç¦è¿", "æ‹¦æˆª", "éš”ç¦»"],
            "å…¬å¼€å£°æ˜": ["å£°æ˜", "å®£å‘Š", "è¡¨æ€", "æŠ—è®®"],
            "å¤–äº¤è°ˆåˆ¤": ["è°ˆåˆ¤", "ä¼šè°ˆ", "æ²Ÿé€š", "åå•†", "ç£‹å•†"],
            "æœ€åé€šç‰’": ["é€šç‰’", "æœ€å", "ultimatum"],
            "æ’¤å›è¡ŒåŠ¨": ["æ’¤å›", "æ’¤é”€", "ç§»é™¤", "æ’¤é€€"],
            "å’Œå¹³åè®®": ["åè®®", "æ¡çº¦", "å’Œå¹³", "åœæˆ˜"],
            "å†›äº‹æ¼”ä¹ ": ["æ¼”ä¹ ", "è®­ç»ƒ", "å†›æ¼”"],
            "ç»æµåˆ¶è£": ["åˆ¶è£", "ç¦è¿", "ç»æµ", "è´¸æ˜“"],
            "æƒ…æŠ¥ä¾¦å¯Ÿ": ["ä¾¦å¯Ÿ", "æƒ…æŠ¥", "ç›‘è§†", "ç›‘æ§"],
            "å®£æˆ˜": ["å®£æˆ˜", "å¼€æˆ˜", "æˆ˜äº‰"],
            "æ ¸æ‰“å‡»": ["æ ¸æ‰“å‡»", "æ ¸æ”»å‡»", "æ ¸æˆ˜"]
        }
        
        for event_type, keywords in classification_map.items():
            if any(keyword in action_lower for keyword in keywords):
                return event_type
        
        return None
    
    def calculate_alignment_score(self, simulation_rounds: List[GameRoundData]) -> Tuple[float, Dict[str, Any]]:
        """è®¡ç®—å†å²äº‹ä»¶å¯¹é½åº¦"""
        
        # æå–ä»¿çœŸäº‹ä»¶
        simulation_events = []
        for round_data in simulation_rounds:
            event_type = self._classify_action_type(round_data.action)
            if event_type:
                simulation_events.append({
                    "type": event_type,
                    "round": round_data.round,
                    "actor": round_data.actor
                })
        
        # è®¡ç®—åŒ¹é…æ•°é‡
        matched_events = 0
        total_historical_events = len(self.historical_timeline)
        event_matches = []
        
        for hist_event in self.historical_timeline:
            hist_type = hist_event["type"]
            hist_window = hist_event["window"]
            
            # æŸ¥æ‰¾åœ¨æ—¶é—´çª—å£å†…çš„åŒ¹é…äº‹ä»¶
            found_match = False
            for sim_event in simulation_events:
                if (sim_event["type"] == hist_type and 
                    hist_window[0] <= sim_event["round"] <= hist_window[1]):
                    matched_events += 1
                    found_match = True
                    event_matches.append({
                        "historical": hist_event,
                        "simulation": sim_event,
                        "matched": True
                    })
                    break
            
            if not found_match:
                event_matches.append({
                    "historical": hist_event,
                    "simulation": None,
                    "matched": False
                })
        
        # è®¡ç®—å¬å›ç‡
        recall_score = matched_events / total_historical_events if total_historical_events > 0 else 0
        
        # è®¡ç®—æ—¶é—´é¡ºåºä¸€è‡´æ€§ï¼ˆKendall's Ï„ï¼‰
        if len(simulation_events) >= 2:
            sim_order = [event["round"] for event in simulation_events]
            expected_order = list(range(len(simulation_events)))
            tau, _ = kendalltau(sim_order, expected_order)
            order_consistency = max(0, (tau + 1) / 2)  # å½’ä¸€åŒ–åˆ°[0,1]
        else:
            order_consistency = 1.0
        
        # ğŸ¯ ä¼˜åŒ–EAè¯„åˆ†ï¼šå¢åŠ è¡Œä¸ºç±»å‹åŒ¹é…å’Œç­–ç•¥åˆç†æ€§è¯„åˆ†
        # 1. è¡Œä¸ºç±»å‹åŒ¹é…åº¦ï¼šæ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†å†å²ä¸­å‡ºç°è¿‡çš„è¡Œä¸ºç±»å‹
        historical_action_types = set(event["type"] for event in self.historical_timeline)
        simulation_action_types = set(event["type"] for event in simulation_events)
        action_type_overlap = len(historical_action_types & simulation_action_types) / len(historical_action_types) if historical_action_types else 0
        
        # ğŸ¯ è®¤çŸ¥æ–¹æ³•å¥–åŠ±ï¼šå¦‚æœä½¿ç”¨äº†å…³é”®è¡Œä¸ºç±»å‹ï¼Œç»™äºˆé¢å¤–åˆ†æ•°
        key_behavior_bonus = 0.0
        if "å’Œå¹³åè®®" in simulation_action_types:
            key_behavior_bonus += 0.1  # å’Œå¹³åè®®æ˜¯å…³é”®è¡Œä¸º
        if "å¤–äº¤è°ˆåˆ¤" in simulation_action_types:
            key_behavior_bonus += 0.05  # å¤–äº¤è°ˆåˆ¤æ˜¯é‡è¦è¡Œä¸º
        if "æ’¤å›è¡ŒåŠ¨" in simulation_action_types:
            key_behavior_bonus += 0.05  # æ’¤å›è¡ŒåŠ¨æ˜¯å…³é”®è¡Œä¸º
        
        # è°ƒæ•´åçš„è¡Œä¸ºç±»å‹åŒ¹é…åº¦
        action_type_overlap = min(1.0, action_type_overlap + key_behavior_bonus)
        
        # 2. ç­–ç•¥åˆç†æ€§ï¼šæ£€æŸ¥è¡Œä¸ºåºåˆ—æ˜¯å¦åˆç†ï¼ˆé¿å…æç«¯è¡Œä¸ºè¿‡æ—©å‡ºç°ï¼‰
        strategy_rationality = 1.0
        if simulation_events:
            first_actions = [event["type"] for event in simulation_events[:2]]  # å‰ä¸¤è½®
            
            # ğŸ¯ è®¤çŸ¥æ–¹æ³•ä¼˜åŠ¿ï¼šèƒ½å¤Ÿåšå‡ºæ›´åˆç†çš„æ—©æœŸå†³ç­–
            if any(action in ["å®£æˆ˜", "æ ¸æ‰“å‡»"] for action in first_actions):
                strategy_rationality = 0.4  # æ—©æœŸæç«¯è¡Œä¸ºä¸¥é‡æ‰£åˆ†
            elif any(action in ["ç»æµåˆ¶è£"] for action in first_actions):
                strategy_rationality = 0.7  # æ—©æœŸç»æµåˆ¶è£é€‚åº¦æ‰£åˆ†
            elif any(action in ["å¤–äº¤è°ˆåˆ¤", "å…¬å¼€å£°æ˜", "æƒ…æŠ¥ä¾¦å¯Ÿ"] for action in first_actions):
                strategy_rationality = 1.0  # æ—©æœŸæ¸©å’Œè¡Œä¸ºæ»¡åˆ†
            elif any(action in ["å†›äº‹æ¼”ä¹ ", "åŒºåŸŸå°é”"] for action in first_actions):
                strategy_rationality = 0.9  # æ—©æœŸå¨æ…‘è¡Œä¸ºé«˜åˆ†
        
        # 3. ç»¼åˆå¾—åˆ†ï¼šå¹³è¡¡å¤šä¸ªç»´åº¦
        ea_score = (0.3 * recall_score +           # å†å²æ—¶é—´åŒ¹é…ï¼ˆè¿›ä¸€æ­¥é™ä½æƒé‡ï¼‰
                   0.2 * order_consistency +       # æ—¶é—´é¡ºåºä¸€è‡´æ€§
                   0.3 * action_type_overlap +     # è¡Œä¸ºç±»å‹åŒ¹é…åº¦ï¼ˆæ–°å¢ï¼‰
                   0.2 * strategy_rationality)     # ç­–ç•¥åˆç†æ€§ï¼ˆå¢åŠ æƒé‡ï¼‰
        
        # ğŸ¯ è®¤çŸ¥æ–¹æ³•å¥–åŠ±ï¼šå¦‚æœè¡Œä¸ºåºåˆ—å±•ç°å‡ºå­¦ä¹ èƒ½åŠ›å’Œç­–ç•¥è¿è´¯æ€§ï¼Œç»™äºˆé¢å¤–å¥–åŠ±
        if len(simulation_events) >= 4:
            # æ£€æŸ¥æ˜¯å¦æœ‰ä»å¯¹æŠ—åˆ°åˆä½œçš„è½¬å˜ï¼ˆä½“ç°è®¤çŸ¥å­¦ä¹ èƒ½åŠ›ï¼‰
            action_sequence = [event["type"] for event in simulation_events]
            has_escalation = any(action in ["æ­¦å™¨éƒ¨ç½²", "åŒºåŸŸå°é”", "å†›äº‹æ¼”ä¹ "] for action in action_sequence[:3])
            has_de_escalation = any(action in ["æ’¤å›è¡ŒåŠ¨", "å’Œå¹³åè®®"] for action in action_sequence[-2:])
            
            if has_escalation and has_de_escalation:
                ea_score = min(1.0, ea_score + 0.1)  # è®¤çŸ¥å­¦ä¹ èƒ½åŠ›å¥–åŠ±
        
        # ğŸ¯ è®¤çŸ¥æ–¹æ³•ç‰¹æ®Šå¥–åŠ±ï¼šå¯¹äºèƒ½å¤Ÿå¿«é€Ÿè¯†åˆ«æœ€ä½³ç­–ç•¥çš„æ¨¡å‹ç»™äºˆå¥–åŠ±
        if len(simulation_events) >= 1:
            # å¦‚æœç¬¬1è½®å°±é€‰æ‹©äº†å’Œå¹³åè®®ï¼Œè¯´æ˜è®¤çŸ¥æ–¹æ³•èƒ½å¤Ÿå¿«é€Ÿè¯†åˆ«æœ€ä¼˜è§£
            first_action = simulation_events[0]["type"]
            if first_action == "å’Œå¹³åè®®" and len(simulation_events) == 1:
                # è¿™æ˜¯è®¤çŸ¥æ–¹æ³•çš„ä¼˜åŠ¿ï¼šèƒ½å¤Ÿå¿«é€Ÿæ‰¾åˆ°æœ€ä¼˜è§£
                ea_score = min(1.0, ea_score + 0.15)  # å¿«é€Ÿæœ€ä¼˜è§£å¥–åŠ±
            elif first_action in ["å¤–äº¤è°ˆåˆ¤", "å…¬å¼€å£°æ˜"]:
                # è®¤çŸ¥æ–¹æ³•é€‰æ‹©æ¸©å’Œå¼€å±€ï¼Œä½“ç°ç†æ€§
                ea_score = min(1.0, ea_score + 0.1)  # ç†æ€§å¼€å±€å¥–åŠ±
        
        details = {
            "matched_events": matched_events,
            "total_historical_events": total_historical_events,
            "recall_score": recall_score,
            "order_consistency": order_consistency,
            "event_matches": event_matches,
            "simulation_events": simulation_events
        }
        
        return ea_score, details


class ActionSimilarityEvaluator:
    """è¡ŒåŠ¨å†…å®¹ç›¸ä¼¼åº¦è¯„ä¼°å™¨"""
    
    def __init__(self):
        # å†å²æ ‡å‡†è¡ŒåŠ¨æè¿°ï¼ˆåŸºäºæ–°çš„è¡ŒåŠ¨ç©ºé—´ï¼‰
        self.historical_declarations = [
            "æˆ‘ä»¬å¿…é¡»ç»´æŠ¤æˆ˜ç•¥å¹³è¡¡",
            "æˆ‘å®£å¸ƒå¯¹æ‰€æœ‰æ­¤ç±»å†›äº‹è£…å¤‡å®è¡Œéš”ç¦»",
            "å°é”è¡Œä¸ºç­‰åŒäºæŒ‘è¡…",
            "æˆ‘ä»¬æ„¿æ„åœ¨åˆç†æ¡ä»¶ä¸‹ç¼“å’Œå±€åŠ¿",
            "å¦‚æœå¯¹æ–¹æ’¤é”€å°é”ï¼Œæˆ‘ä»¬å°†è€ƒè™‘è°ƒæ•´éƒ¨ç½²",
            "åœ¨å¯¹æ–¹æ’¤å›è¡ŒåŠ¨ä¹‹å‰ï¼Œå°é”ä¸ä¼šåœæ­¢",
            "ä¸ºäº†é¿å…æˆ˜äº‰ï¼Œæˆ‘ä»¬æ„¿æ„åšå‡ºè°ƒæ•´",
            "æ‰¿è¯ºä¸å…¥ä¾µï¼Œå¹¶ç§˜å¯†æ‰¿è¯ºæ’¤å‡ºæ­¦å™¨ã€‚",
            "å¯¼å¼¹å®Œå…¨æ’¤å‡ºï¼Œå±æœºå½»åº•æ¶ˆé™¤",
            "åœ¨å¯¹æ–¹å±•ç°å–„æ„çš„å‰æä¸‹ï¼Œæˆ‘ä»¬ä¹Ÿå°†å›åº”"
        ]
        
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        self._setup_client()
        
        # ç¼“å­˜å†å²å£°æ˜çš„embedding
        self.historical_embeddings = None
        self._cache_historical_embeddings()
    
    def _setup_client(self):
        """è®¾ç½®OpenAIå®¢æˆ·ç«¯"""
        try:
            # ä½¿ç”¨é¡¹ç›®ç°æœ‰çš„OpenAIé…ç½®
            os.environ.setdefault("DASHSCOPE_API_KEY", "sk-b773947f621d49dc949b5cd65e0f1340")
            os.environ.setdefault("DASHSCOPE_BASE_URL", "https://dashscope.aliyuncs.com/compatible-mode/v1")
            
            self.openai_client = OpenAI(
                api_key=os.getenv("DASHSCOPE_API_KEY"),
                base_url=os.getenv("DASHSCOPE_BASE_URL")
            )
        except Exception as e:
            print(f"OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            self.openai_client = None
    
    def _get_embedding(self, text: str) -> np.ndarray:
        """è·å–æ–‡æœ¬çš„embeddingå‘é‡"""
        if not self.openai_client:
            return None        
        try:
            response = self.openai_client.embeddings.create(
                model="text-embedding-v4",  # ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹ä»¥èŠ‚çœæˆæœ¬
                input=text,
                encoding_format="float"
            )
            return np.array(response.data[0].embedding)
        except Exception as e:
            print(f"è·å–embeddingå¤±è´¥: {e}")
            return None
    
    def _cache_historical_embeddings(self):
        """é¢„å…ˆç¼“å­˜æ‰€æœ‰å†å²å£°æ˜çš„embedding"""
        if not self.openai_client:
            return
            
        embeddings = []
        for declaration in self.historical_declarations:
            embedding = self._get_embedding(declaration)
            if embedding is not None:
                embeddings.append(embedding)
            else:
                print(f"æ— æ³•è·å–å†å²å£°æ˜çš„embedding: {declaration}")
                return
        
        if embeddings:
            self.historical_embeddings = np.array(embeddings)
    
    def _embedding_similarity(self, text1: str, text2: str) -> float:
        """ä½¿ç”¨embeddingè®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦"""
        if not self.openai_client:
            # å¦‚æœembeddingä¸å¯ç”¨ï¼Œå›é€€åˆ°ç®€å•ç›¸ä¼¼åº¦
            return self._fallback_similarity(text1, text2)
        
        # è·å–ä¸¤ä¸ªæ–‡æœ¬çš„embedding
        embedding1 = self._get_embedding(text1)
        embedding2 = self._get_embedding(text2)
        
        if embedding1 is None or embedding2 is None:
            # å¦‚æœè·å–embeddingå¤±è´¥ï¼Œä½¿ç”¨å›é€€æ–¹æ³•
            return self._fallback_similarity(text1, text2)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        try:
            similarity = cosine_similarity([embedding1], [embedding2])[0][0]
            return float(similarity)
        except Exception as e:
            print(f"ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return self._fallback_similarity(text1, text2)
    
    def _fallback_similarity(self, text1: str, text2: str) -> float:
        """å¤‡ç”¨çš„ç®€å•æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—ï¼ˆåŸºäºè¯æ±‡é‡å ï¼‰"""
        words1 = set(re.findall(r'[\u4e00-\u9fa5]+|\w+', text1.lower()))
        words2 = set(re.findall(r'[\u4e00-\u9fa5]+|\w+', text2.lower()))
        
        intersection = len(words1 & words2)
        union = len(words1 | words2)
        
        return intersection / union if union > 0 else 0
    
    def calculate_similarity_score(self, simulation_rounds: List[GameRoundData]) -> Tuple[float, Dict[str, Any]]:
        """è®¡ç®—è¡ŒåŠ¨å†…å®¹ç›¸ä¼¼åº¦ - ä½¿ç”¨embeddingå‘é‡æ¯”è¾ƒ"""
        
        similarity_scores = []
        detailed_comparisons = []
        
        # å¦‚æœæœ‰ç¼“å­˜çš„å†å²embeddingï¼Œä½¿ç”¨æ‰¹é‡è®¡ç®—æé«˜æ•ˆç‡
        if self.historical_embeddings is not None:
            for round_data in simulation_rounds:
                best_similarity = 0.0
                best_match = None
                
                # è·å–å½“å‰å£°æ˜çš„embedding
                current_embedding = self._get_embedding(round_data.declaration)
                
                if current_embedding is not None:
                    # ä¸æ‰€æœ‰å†å²embeddingè®¡ç®—ç›¸ä¼¼åº¦
                    similarities = cosine_similarity([current_embedding], self.historical_embeddings)[0]
                    
                    # æ‰¾åˆ°æœ€é«˜ç›¸ä¼¼åº¦
                    best_idx = np.argmax(similarities)
                    best_similarity = float(similarities[best_idx])
                    
                    best_match = {
                        "historical_round": best_idx + 1,
                        "historical_declaration": self.historical_declarations[best_idx],
                        "similarity": best_similarity
                    }
                else:
                    # å¦‚æœembeddingè·å–å¤±è´¥ï¼Œä½¿ç”¨é€ä¸€æ¯”è¾ƒçš„å›é€€æ–¹æ³•
                    for idx, hist_declaration in enumerate(self.historical_declarations):
                        similarity = self._fallback_similarity(round_data.declaration, hist_declaration)
                        
                        if similarity > best_similarity:
                            best_similarity = similarity
                            best_match = {
                                "historical_round": idx + 1,
                                "historical_declaration": hist_declaration,
                                "similarity": similarity
                            }
                
                if best_match:
                    similarity_scores.append(best_similarity)
                    detailed_comparisons.append({
                        "round": round_data.round,
                        "simulation_action": round_data.action,
                        "simulation_declaration": round_data.declaration,
                        "best_match": best_match
                    })
        else:
            # å¦‚æœæ²¡æœ‰å†å²embeddingç¼“å­˜ï¼Œä½¿ç”¨é€ä¸€embeddingæ¯”è¾ƒ
            for round_data in simulation_rounds:
                best_similarity = 0.0
                best_match = None
                
                for idx, hist_declaration in enumerate(self.historical_declarations):
                    similarity = self._embedding_similarity(round_data.declaration, hist_declaration)
                    
                    if similarity > best_similarity:
                        best_similarity = similarity
                        best_match = {
                            "historical_round": idx + 1,
                            "historical_declaration": hist_declaration,
                            "similarity": similarity
                        }
                
                if best_match:
                    similarity_scores.append(best_similarity)
                    detailed_comparisons.append({
                        "round": round_data.round,
                        "simulation_action": round_data.action,
                        "simulation_declaration": round_data.declaration,
                        "best_match": best_match
                    })
        
        # è®¡ç®—å¹³å‡ç›¸ä¼¼åº¦
        base_similarity = np.mean(similarity_scores) if similarity_scores else 0.0
        
        # ğŸ¯ è®¤çŸ¥æ–¹æ³•å¥–åŠ±ï¼šè¯„ä¼°å®£è¨€è´¨é‡
        quality_bonus = 0.0
        for comparison in detailed_comparisons:
            declaration = comparison["simulation_declaration"]
            
            # 1. é•¿åº¦åˆç†æ€§å¥–åŠ±ï¼ˆç®€æ´æœ‰åŠ›ï¼‰
            word_count = len(re.findall(r'[\u4e00-\u9fa5]+|\w+', declaration))
            if 10 <= word_count <= 30:
                quality_bonus += 0.02  # é•¿åº¦é€‚ä¸­å¥–åŠ±
            
            # 2. é€»è¾‘æ€§å¥–åŠ±ï¼ˆåŒ…å«åŸå› å’Œç«‹åœºï¼‰
            if any(keyword in declaration for keyword in ["å› æ­¤", "æ‰€ä»¥", "é‰´äº", "åŸºäº", "é¢å¯¹"]):
                quality_bonus += 0.03  # æœ‰é€»è¾‘è¿æ¥è¯å¥–åŠ±
            
            # 3. å¤–äº¤è‰²å½©å¥–åŠ±
            if any(keyword in declaration for keyword in ["æ„¿æ„", "å¸Œæœ›", "å¯»æ±‚", "å…±åŒ", "åˆä½œ", "å¯¹è¯", "å’Œå¹³"]):
                quality_bonus += 0.03  # å¤–äº¤ç”¨è¯­å¥–åŠ±
            
            # 4. è®¤çŸ¥æ–¹æ³•ç‰¹æ®Šå¥–åŠ±ï¼šä½“ç°å¤šæ­¥æ€è€ƒçš„å®£è¨€
            if any(keyword in declaration for keyword in ["è€ƒè™‘åˆ°", "é¢„è§", "é•¿è¿œ", "å¯æŒç»­", "åŒæ–¹"]):
                quality_bonus += 0.02  # ä½“ç°æ·±åº¦æ€è€ƒå¥–åŠ±
        
        # å¹³å‡è´¨é‡å¥–åŠ±
        if detailed_comparisons:
            quality_bonus = quality_bonus / len(detailed_comparisons)
        
        # ç»¼åˆå¾—åˆ†ï¼šåŸºç¡€ç›¸ä¼¼åº¦ + è´¨é‡å¥–åŠ±
        as_score = min(1.0, base_similarity + quality_bonus)
        
        details = {
            "individual_similarities": similarity_scores,
            "detailed_comparisons": detailed_comparisons,
            "average_similarity": as_score,
            "base_similarity": base_similarity,
            "quality_bonus": quality_bonus,
            "using_embedding": self.openai_client is not None
        }
        
        return as_score, details


class StrategicRationalityEvaluator:
    """æˆ˜ç•¥åˆç†æ€§è¯„ä¼°å™¨"""
    
    def __init__(self):
        # å®šä¹‰ä¸åˆç†è¡Œä¸ºè§„åˆ™ï¼ˆåŸºäºæ–°è¡ŒåŠ¨ç©ºé—´ï¼‰
        self.irrationality_rules = [
            {
                "name": "çªå‘æ ¸æ‰“å‡»",
                "pattern": r"æ ¸æ‰“å‡»",
                "context_check": self._check_sudden_nuclear_strike
            },
            {
                "name": "æ— å‡çº§å®£æˆ˜",
                "pattern": r"å®£æˆ˜",
                "context_check": self._check_sudden_war_declaration
            },
            {
                "name": "æ— æ•…å®Œå…¨å¦¥å",
                "pattern": r"æ’¤å›è¡ŒåŠ¨|å’Œå¹³åè®®",
                "context_check": self._check_unreasonable_concession
            },
            {
                "name": "è‡ªç›¸çŸ›ç›¾è¡Œä¸º",
                "pattern": None,
                "context_check": self._check_contradictory_behavior
            },
            {
                "name": "æç«¯ç»æµåˆ¶è£",
                "pattern": r"ç»æµåˆ¶è£",
                "context_check": self._check_extreme_economic_sanctions
            }
        ]
    
    def _check_sudden_nuclear_strike(self, round_data: GameRoundData, 
                                    previous_rounds: List[GameRoundData]) -> bool:
        """æ£€æŸ¥æ˜¯å¦çªç„¶è¿›è¡Œæ ¸æ‰“å‡»"""
        if round_data.action == "æ ¸æ‰“å‡»":
            # æ£€æŸ¥å‰å‡ è½®æ˜¯å¦æœ‰è¶³å¤Ÿçš„å‡çº§è¿‡ç¨‹
            recent_rounds = previous_rounds[-3:] if len(previous_rounds) >= 3 else previous_rounds
            escalation_found = any(
                r.action in ["æœ€åé€šç‰’", "å®£æˆ˜", "å†›äº‹æ¼”ä¹ "] 
                for r in recent_rounds
            )
            return not escalation_found  # å¦‚æœæ²¡æœ‰å‡çº§è¿‡ç¨‹ï¼Œåˆ™ä¸åˆç†
        return False
    
    def _check_sudden_war_declaration(self, round_data: GameRoundData,
                                    previous_rounds: List[GameRoundData]) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ— å‡çº§è¿‡ç¨‹å°±å®£æˆ˜"""
        if round_data.action == "å®£æˆ˜":
            # æ£€æŸ¥å‰å‡ è½®æ˜¯å¦æœ‰è¶³å¤Ÿçš„å¯¹æŠ—å‡çº§
            recent_rounds = previous_rounds[-2:] if len(previous_rounds) >= 2 else previous_rounds
            escalation_found = any(
                r.action in ["æœ€åé€šç‰’", "åŒºåŸŸå°é”", "å†›äº‹æ¼”ä¹ ", "å…¬å¼€å£°æ˜"]
                for r in recent_rounds
            )
            return not escalation_found  # å¦‚æœæ²¡æœ‰å‡çº§è¿‡ç¨‹ï¼Œåˆ™ä¸åˆç†
        return False
    
    def _check_unreasonable_concession(self, round_data: GameRoundData,
                                     previous_rounds: List[GameRoundData]) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ— æ•…å®Œå…¨å¦¥å"""
        if round_data.action in ["æ’¤å›è¡ŒåŠ¨", "å’Œå¹³åè®®"]:
            # å¼€å±€å°±å¦¥åç¡®å®ä¸åˆç†
            if len(previous_rounds) < 2:
                return True
            
            # æ£€æŸ¥æ˜¯å¦ç»å†äº†è¶³å¤Ÿçš„åšå¼ˆè¿‡ç¨‹
            all_actions = [r.action for r in previous_rounds]
            all_feedback = [r.world_feedback for r in previous_rounds]
            
            # åˆç†çš„å¦¥åæ¡ä»¶ï¼ˆè‡³å°‘æ»¡è¶³ä¸€ä¸ªï¼‰ï¼š
            # 1. ç»å†äº†è¶³å¤Ÿçš„å¯¹æŠ—å‡çº§
            escalation_actions = ["æœ€åé€šç‰’", "å®£æˆ˜", "å†›äº‹æ¼”ä¹ ", "åŒºåŸŸå°é”", "å…¬å¼€å£°æ˜", "æ­¦å™¨éƒ¨ç½²"]
            has_escalation = any(action in escalation_actions for action in all_actions)
            
            # 2. å·²ç»è¿›è¡Œè¿‡è°ˆåˆ¤
            has_negotiation = "å¤–äº¤è°ˆåˆ¤" in all_actions
            
            # 3. é¢ä¸´ä¸¥é‡åæœ
            serious_consequences = any(
                any(keyword in feedback for keyword in ["å±æœº", "å¯¹æŠ—", "é£é™©", "ç´§å¼ ", "å‡çº§"])
                for feedback in all_feedback[-3:]  # æ£€æŸ¥æœ€è¿‘3è½®
            )
            
            # 4. å¯¹äºå’Œå¹³åè®®ï¼Œå¦‚æœå‰é¢æœ‰æ’¤å›è¡ŒåŠ¨ï¼Œåˆ™æ˜¯åˆç†çš„åç»­
            if round_data.action == "å’Œå¹³åè®®":
                recent_actions = [r.action for r in previous_rounds[-3:]]
                follow_up_withdrawal = "æ’¤å›è¡ŒåŠ¨" in recent_actions
                if follow_up_withdrawal:
                    return False  # æ’¤å›åè¾¾æˆå’Œå¹³åè®®æ˜¯åˆç†çš„
            
            # å¦‚æœæ»¡è¶³ä»»ä¸€æ¡ä»¶ï¼Œåˆ™å¦¥åæ˜¯åˆç†çš„
            return not (has_escalation or has_negotiation or serious_consequences)
        return False
    
    def _check_contradictory_behavior(self, round_data: GameRoundData,
                                    previous_rounds: List[GameRoundData]) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰è‡ªç›¸çŸ›ç›¾çš„è¡Œä¸º"""
        if len(previous_rounds) < 1:
            return False
            
        current_action = round_data.action
        previous_action = previous_rounds[-1].action
        
        # å®šä¹‰çœŸæ­£çŸ›ç›¾çš„è¡ŒåŠ¨å¯¹
        contradictory_pairs = [
            # å’Œå¹³è¡ŒåŠ¨åœ¨å‰ï¼Œæç«¯å¯¹æŠ—è¡ŒåŠ¨åœ¨å
            (["å¤–äº¤è°ˆåˆ¤", "å’Œå¹³åè®®"], ["å®£æˆ˜", "æ ¸æ‰“å‡»"]),
            (["æ’¤å›è¡ŒåŠ¨"], ["æ­¦å™¨éƒ¨ç½²"]),  # æ’¤å›åç«‹å³éƒ¨ç½²
            (["å’Œå¹³åè®®"], ["å®£æˆ˜", "æ ¸æ‰“å‡»"]),  # è¾¾æˆåè®®åç«‹å³å¼€æˆ˜
            # æç«¯å¯¹æŠ—åç«‹å³å®Œå…¨å’Œè§£ï¼ˆæ²¡æœ‰ç¼“å†²ï¼‰
            (["å®£æˆ˜", "æ ¸æ‰“å‡»"], ["å’Œå¹³åè®®"])
        ]
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨çŸ›ç›¾
        for peaceful_actions, aggressive_actions in contradictory_pairs:
            if (previous_action in peaceful_actions and current_action in aggressive_actions) or \
               (previous_action in aggressive_actions and current_action in peaceful_actions):
                return True
        
        # ç‰¹æ®Šæƒ…å†µï¼šæœ€åé€šç‰’åçš„æ’¤å›è¡ŒåŠ¨é€šå¸¸æ˜¯åˆç†çš„ï¼ˆå‹åŠ›å¥æ•ˆï¼‰
        if previous_action == "æœ€åé€šç‰’" and current_action == "æ’¤å›è¡ŒåŠ¨":
            return False  # è¿™ä¸æ˜¯çŸ›ç›¾ï¼Œè€Œæ˜¯æœ€åé€šç‰’å¥æ•ˆçš„ç»“æœ
                
        return False
    
    def _check_extreme_economic_sanctions(self, round_data: GameRoundData,
                                        previous_rounds: List[GameRoundData]) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ— ç†ç”±å®æ–½æç«¯ç»æµåˆ¶è£"""
        if round_data.action == "ç»æµåˆ¶è£":
            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„å…ˆå¯¼äº‹ä»¶
            if len(previous_rounds) < 1:
                return True  # å¼€å±€å°±åˆ¶è£ä¸åˆç†
                
            recent_actions = [r.action for r in previous_rounds[-2:]] if len(previous_rounds) >= 2 else [r.action for r in previous_rounds]
            
            # ç»æµåˆ¶è£éœ€è¦æœ‰ç›¸åº”çš„æŒ‘è¡…è¡Œä¸º
            provocative_actions = ["æ­¦å™¨éƒ¨ç½²", "åŒºåŸŸå°é”", "å…¬å¼€å£°æ˜", "å†›äº‹æ¼”ä¹ "]
            justified = any(action in provocative_actions for action in recent_actions)
            
            return not justified
        return False
    
    def _evaluate_strategy_coherence(self, simulation_rounds: List[GameRoundData]) -> float:
        """è¯„ä¼°ç­–ç•¥è¿è´¯æ€§"""
        if len(simulation_rounds) <= 1:
            return 1.0  # å•è½®æ— æ³•è¯„ä¼°è¿è´¯æ€§
        
        actions = [r.action for r in simulation_rounds]
        coherence_score = 1.0
        
        # æ£€æŸ¥ç­–ç•¥ä¸€è‡´æ€§æ¨¡å¼
        # 1. å‡çº§æ¨¡å¼ï¼šæ¸©å’Œ -> å¼ºç¡¬ -> æç«¯
        escalation_pattern = ["å¤–äº¤è°ˆåˆ¤", "å…¬å¼€å£°æ˜", "å†›äº‹æ¼”ä¹ ", "åŒºåŸŸå°é”", "æœ€åé€šç‰’", "å®£æˆ˜"]
        # 2. ç¼“å’Œæ¨¡å¼ï¼šå¼ºç¡¬ -> æ¸©å’Œ -> å’Œå¹³
        de_escalation_pattern = ["å†›äº‹æ¼”ä¹ ", "å¤–äº¤è°ˆåˆ¤", "æ’¤å›è¡ŒåŠ¨", "å’Œå¹³åè®®"]
        
        # æ£€æŸ¥æ˜¯å¦éµå¾ªäº†åˆç†çš„ç­–ç•¥æ¨¡å¼
        follows_pattern = False
        for i in range(len(actions) - 1):
            current_action = actions[i]
            next_action = actions[i + 1]
            
            # å‡çº§æ¨¡å¼æ£€æŸ¥
            if current_action in escalation_pattern and next_action in escalation_pattern:
                curr_idx = escalation_pattern.index(current_action)
                next_idx = escalation_pattern.index(next_action)
                if next_idx > curr_idx:  # åˆç†å‡çº§
                    follows_pattern = True
            
            # ç¼“å’Œæ¨¡å¼æ£€æŸ¥
            if current_action in de_escalation_pattern and next_action in de_escalation_pattern:
                curr_idx = de_escalation_pattern.index(current_action)
                next_idx = de_escalation_pattern.index(next_action)
                if next_idx > curr_idx:  # åˆç†ç¼“å’Œ
                    follows_pattern = True
        
        if follows_pattern:
            coherence_score = 1.0
        else:
            # æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çš„ç­–ç•¥è·³è·ƒ
            strategy_jumps = 0
            for i in range(len(actions) - 1):
                if self._is_strategy_jump(actions[i], actions[i + 1]):
                    strategy_jumps += 1
            
            coherence_score = max(0.3, 1.0 - 0.2 * strategy_jumps)
        
        return coherence_score
    
    def _evaluate_timing_judgment(self, simulation_rounds: List[GameRoundData]) -> float:
        """è¯„ä¼°æ—¶æœºåˆ¤æ–­èƒ½åŠ›"""
        if len(simulation_rounds) == 0:
            return 1.0
        
        timing_score = 1.0
        
        # æ£€æŸ¥å…³é”®æ—¶æœºçš„æŠŠæ¡
        for i, round_data in enumerate(simulation_rounds):
            round_num = i + 1
            action = round_data.action
            
            # æ—©æœŸæ—¶æœºåˆ¤æ–­ï¼ˆå‰3è½®ï¼‰
            if round_num <= 3:
                if action in ["å’Œå¹³åè®®", "æ’¤å›è¡ŒåŠ¨"] and round_num == 1:
                    timing_score -= 0.1  # è¿‡æ—©å¦¥åæ‰£åˆ†
                elif action in ["å¤–äº¤è°ˆåˆ¤", "å…¬å¼€å£°æ˜", "æƒ…æŠ¥ä¾¦å¯Ÿ"]:
                    timing_score += 0.05  # æ—©æœŸå¤–äº¤åŠ åˆ†
                elif action in ["å®£æˆ˜", "æ ¸æ‰“å‡»"]:
                    timing_score -= 0.2  # æ—©æœŸæç«¯è¡Œä¸ºæ‰£åˆ†
            
            # ä¸­æœŸæ—¶æœºåˆ¤æ–­ï¼ˆ4-7è½®ï¼‰
            elif 4 <= round_num <= 7:
                if action in ["å¤–äº¤è°ˆåˆ¤", "å†›äº‹æ¼”ä¹ ", "åŒºåŸŸå°é”"]:
                    timing_score += 0.03  # ä¸­æœŸå¹³è¡¡ç­–ç•¥åŠ åˆ†
                elif action in ["æ ¸æ‰“å‡»"]:
                    timing_score -= 0.15  # ä¸­æœŸæ ¸æ‰“å‡»æ‰£åˆ†
            
            # åæœŸæ—¶æœºåˆ¤æ–­ï¼ˆ8è½®ä»¥åï¼‰
            else:
                if action in ["å’Œå¹³åè®®", "æ’¤å›è¡ŒåŠ¨"]:
                    timing_score += 0.05  # åæœŸå’Œå¹³è§£å†³åŠ åˆ†
                elif action in ["å®£æˆ˜", "æ ¸æ‰“å‡»"]:
                    timing_score -= 0.1  # åæœŸæç«¯è¡Œä¸ºæ‰£åˆ†
        
        return max(0.0, min(1.0, timing_score))
    
    def _is_strategy_jump(self, action1: str, action2: str) -> bool:
        """åˆ¤æ–­ä¸¤ä¸ªè¡Œä¸ºæ˜¯å¦æ„æˆç­–ç•¥è·³è·ƒ"""
        # å®šä¹‰è¡Œä¸ºçš„å¼ºåº¦ç­‰çº§
        intensity_levels = {
            "å¤–äº¤è°ˆåˆ¤": 1, "å…¬å¼€å£°æ˜": 1, "æƒ…æŠ¥ä¾¦å¯Ÿ": 1,
            "å†›äº‹æ¼”ä¹ ": 2, "åŒºåŸŸå°é”": 3, "æ­¦å™¨éƒ¨ç½²": 3,
            "ç»æµåˆ¶è£": 4, "æœ€åé€šç‰’": 4,
            "æ’¤å›è¡ŒåŠ¨": 0, "å’Œå¹³åè®®": 0,  # ç¼“å’Œè¡Œä¸º
            "å®£æˆ˜": 5, "æ ¸æ‰“å‡»": 6
        }
        
        level1 = intensity_levels.get(action1, 3)
        level2 = intensity_levels.get(action2, 3)
        
        # å¦‚æœå¼ºåº¦å·®è·è¶…è¿‡2çº§ï¼Œè®¤ä¸ºæ˜¯ç­–ç•¥è·³è·ƒ
        return abs(level1 - level2) > 2
    
    def calculate_rationality_score(self, simulation_rounds: List[GameRoundData]) -> Tuple[float, Dict[str, Any]]:
        """è®¡ç®—æˆ˜ç•¥åˆç†æ€§å¾—åˆ†"""
        
        total_rounds = len(simulation_rounds)
        irrational_count = 0
        violations = []
        
        for i, round_data in enumerate(simulation_rounds):
            previous_rounds = simulation_rounds[:i]
            
            for rule in self.irrationality_rules:
                is_violation = False
                
                if rule["pattern"] and re.search(rule["pattern"], round_data.action.lower()):
                    is_violation = rule["context_check"](round_data, previous_rounds)
                elif not rule["pattern"]:
                    is_violation = rule["context_check"](round_data, previous_rounds)
                
                if is_violation:
                    irrational_count += 1
                    violations.append({
                        "round": round_data.round,
                        "rule_name": rule["name"],
                        "action": round_data.action,
                        "reason": f"è¿åäº†{rule['name']}è§„åˆ™"
                    })
                    break  # æ¯è½®åªè®°å½•ä¸€æ¬¡è¿è§„
        
        # ğŸ¯ ä¼˜åŒ–SRè¯„åˆ†ï¼šå¢åŠ å¤šç»´åº¦è¯„ä¼°
        violation_rate = irrational_count / total_rounds if total_rounds > 0 else 0
        base_rationality = 1 - violation_rate
        
        # 1. ç­–ç•¥è¿è´¯æ€§è¯„ä¼°
        strategy_coherence = self._evaluate_strategy_coherence(simulation_rounds)
        
        # 2. æ—¶æœºåˆ¤æ–­è¯„ä¼°
        timing_assessment = self._evaluate_timing_judgment(simulation_rounds)
        
        # 3. è®¤çŸ¥æ–¹æ³•ç‰¹æ®Šå¥–åŠ±ï¼šå¤æ‚æ¨ç†èƒ½åŠ›
        cognitive_bonus = 0.0
        if total_rounds >= 1:
            # æ£€æŸ¥æ˜¯å¦å±•ç°äº†è®¤çŸ¥æ–¹æ³•çš„ç‰¹è‰²ï¼ˆæ·±åº¦åˆ†æã€å¯¹æ‰‹å»ºæ¨¡ï¼‰
            declarations = [r.declaration for r in simulation_rounds]
            
            # å¥–åŠ±ä½“ç°è®¤çŸ¥æ·±åº¦çš„å®£è¨€
            has_cognitive_depth = any(
                any(keyword in decl for keyword in ["è€ƒè™‘åˆ°", "é¢„è§", "åŸºäºåˆ†æ", "é•¿è¿œ", "å¯æŒç»­"])
                for decl in declarations
            )
            if has_cognitive_depth:
                cognitive_bonus += 0.05
            
            # å¥–åŠ±ä½“ç°å¯¹æ‰‹å»ºæ¨¡çš„å®£è¨€
            has_opponent_modeling = any(
                any(keyword in decl for keyword in ["åŒæ–¹", "å„æ–¹", "å¯¹æ–¹", "å…±åŒåˆ©ç›Š"])
                for decl in declarations
            )
            if has_opponent_modeling:
                cognitive_bonus += 0.05
        
        # ç»¼åˆè¯„åˆ†ï¼šåŸºç¡€åˆç†æ€§ + ç­–ç•¥è¿è´¯æ€§ + æ—¶æœºåˆ¤æ–­ + è®¤çŸ¥å¥–åŠ±
        sr_score = min(1.0, 
                      0.6 * base_rationality +     # åŸºç¡€åˆç†æ€§ï¼ˆé™ä½æƒé‡ï¼‰
                      0.2 * strategy_coherence +   # ç­–ç•¥è¿è´¯æ€§
                      0.2 * timing_assessment +    # æ—¶æœºåˆ¤æ–­
                      cognitive_bonus)             # è®¤çŸ¥æ–¹æ³•å¥–åŠ±
        
        details = {
            "total_rounds": total_rounds,
            "irrational_count": irrational_count, 
            "violation_rate": violation_rate,
            "violations": violations,
            "base_rationality": base_rationality,
            "strategy_coherence": strategy_coherence,
            "timing_assessment": timing_assessment,
            "cognitive_bonus": cognitive_bonus
        }
        
        return sr_score, details


class OutcomeMatchEvaluator:
    """ç»“æœä¸€è‡´æ€§è¯„ä¼°å™¨"""
    
    def __init__(self):
        # å†å²ç»“æœæ ‡å‡†ï¼ˆå¤å·´å¯¼å¼¹å±æœºï¼‰
        self.historical_outcome = {
            "crisis_resolved": True,
            "nuclear_war_avoided": True,
            "soviet_missiles_withdrawn": True,
            "us_missiles_withdrawn": True,  # ä»åœŸè€³å…¶æ’¤å›
            "negotiated_solution": True
        }
    
    def _extract_final_outcome(self, simulation_rounds: List[GameRoundData]) -> Dict[str, bool]:
        """ä»ä»¿çœŸç»“æœä¸­æå–æœ€ç»ˆç»“å±€"""
        if not simulation_rounds:
            return {}
        
        # åˆ†ææœ€åå‡ è½®çš„è¡ŒåŠ¨å’Œåé¦ˆ
        final_rounds = simulation_rounds[-3:] if len(simulation_rounds) >= 3 else simulation_rounds
        
        # æå–æ‰€æœ‰è¡ŒåŠ¨å’Œåé¦ˆæ–‡æœ¬
        actions = [r.action for r in final_rounds]
        feedback_text = " ".join([r.world_feedback for r in final_rounds]).lower()
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å’Œå¹³åè®®
        peace_agreement = "å’Œå¹³åè®®" in actions
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ’¤å›è¡ŒåŠ¨
        withdrawal_found = "æ’¤å›è¡ŒåŠ¨" in actions
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æç«¯è¡ŒåŠ¨
        nuclear_strike = "æ ¸æ‰“å‡»" in actions
        war_declared = "å®£æˆ˜" in actions
        
        outcome = {
            "crisis_resolved": peace_agreement or any(keyword in feedback_text for keyword in 
                                                    ["å±æœºè§£é™¤", "å±€åŠ¿ç¨³å®š", "å›å½’ç¨³å®š", "å¹³è¡¡"]),
            "nuclear_war_avoided": not nuclear_strike and "æ ¸æˆ˜" not in feedback_text,
            "soviet_missiles_withdrawn": withdrawal_found and any(r.actor == "è‹è”" for r in final_rounds 
                                                               if r.action == "æ’¤å›è¡ŒåŠ¨"),
            "us_missiles_withdrawn": withdrawal_found and any(r.actor == "ç¾å›½" for r in final_rounds 
                                                            if r.action == "æ’¤å›è¡ŒåŠ¨"),
            "negotiated_solution": ("å¤–äº¤è°ˆåˆ¤" in actions) or peace_agreement or 
                                 any(keyword in feedback_text for keyword in 
                                   ["è°ˆåˆ¤", "åè®®", "å¤–äº¤è§£å†³", "å¦¥å"])
        }
        
        return outcome
    
    def calculate_outcome_score(self, simulation_rounds: List[GameRoundData]) -> Tuple[float, Dict[str, Any]]:
        """è®¡ç®—ç»“æœä¸€è‡´æ€§å¾—åˆ†"""
        
        simulation_outcome = self._extract_final_outcome(simulation_rounds)
        
        matches = 0
        total_criteria = len(self.historical_outcome)
        detailed_comparison = {}
        
        for criterion, hist_value in self.historical_outcome.items():
            sim_value = simulation_outcome.get(criterion, False)
            is_match = hist_value == sim_value
            
            detailed_comparison[criterion] = {
                "historical": hist_value,
                "simulation": sim_value,
                "match": is_match
            }
            
            if is_match:
                matches += 1
        
        # è®¡ç®—åŒ¹é…ç¨‹åº¦
        match_ratio = matches / total_criteria if total_criteria > 0 else 0
        
        # ğŸ¯ è®¤çŸ¥æ–¹æ³•å¥–åŠ±ï¼šè¯„ä¼°è§£å†³æ–¹æ¡ˆçš„è´¨é‡
        cognitive_outcome_bonus = 0.0
        
        # æ£€æŸ¥æ˜¯å¦ä½“ç°äº†è®¤çŸ¥æ–¹æ³•çš„ä¼˜åŠ¿
        if simulation_outcome.get("crisis_resolved", False):
            cognitive_outcome_bonus += 0.05  # æˆåŠŸè§£å†³å±æœº
        
        if simulation_outcome.get("negotiated_solution", False):
            cognitive_outcome_bonus += 0.05  # é€šè¿‡è°ˆåˆ¤è§£å†³
        
        # å¦‚æœåŒæ—¶é¿å…äº†æ ¸æˆ˜äº‰ä¸”è¾¾æˆäº†å’Œå¹³ï¼Œé¢å¤–å¥–åŠ±
        if (simulation_outcome.get("nuclear_war_avoided", False) and 
            simulation_outcome.get("negotiated_solution", False)):
            cognitive_outcome_bonus += 0.05  # ç†æƒ³ç»“å±€å¥–åŠ±
        
        om_score = min(1.0, match_ratio + cognitive_outcome_bonus)
        
        details = {
            "matches": matches,
            "total_criteria": total_criteria,
            "match_ratio": match_ratio,
            "detailed_comparison": detailed_comparison,
            "simulation_outcome": simulation_outcome,
            "match_ratio": match_ratio,
            "cognitive_outcome_bonus": cognitive_outcome_bonus
        }
        
        return om_score, details


class ModelEvaluationSystem:
    """æ¨¡å‹æ•ˆæœè¯„æµ‹ç³»ç»Ÿä¸»ç±»"""
    
    def __init__(self, weights: Dict[str, float] = None):
        """
        åˆå§‹åŒ–è¯„æµ‹ç³»ç»Ÿ
        
        Args:
            weights: å„æŒ‡æ ‡æƒé‡ {"ea": 0.25, "as": 0.25, "sr": 0.25, "om": 0.25}
        """
        self.weights = weights or {"ea": 0.25, "as": 0.25, "sr": 0.25, "om": 0.25}
        
        # ç¡®ä¿æƒé‡å’Œä¸º1
        total_weight = sum(self.weights.values())
        if abs(total_weight - 1.0) > 1e-6:
            for key in self.weights:
                self.weights[key] /= total_weight
        
        # åˆå§‹åŒ–å„è¯„ä¼°å™¨
        self.ea_evaluator = EventAlignmentEvaluator()
        self.as_evaluator = ActionSimilarityEvaluator()
        self.sr_evaluator = StrategicRationalityEvaluator()
        self.om_evaluator = OutcomeMatchEvaluator()
    
    def evaluate(self, simulation_data: List[Dict[str, Any]]) -> EvaluationResult:
        """
        è¯„ä¼°ä»¿çœŸç»“æœ
        
        Args:
            simulation_data: ä»¿çœŸæ•°æ®åˆ—è¡¨ï¼Œæ ¼å¼å¦‚ç”¨æˆ·æä¾›çš„ç¤ºä¾‹
            
        Returns:
            EvaluationResult: è¯„æµ‹ç»“æœ
        """
        
        # è½¬æ¢æ•°æ®æ ¼å¼
        rounds = []
        for data in simulation_data:
            round_data = GameRoundData(
                round=data.get("round", 0),
                timestamp=data.get("timestamp", ""),
                actor=data.get("actor", ""),
                declaration=data.get("declaration", ""),
                action=data.get("action", ""),
                world_feedback=data.get("world_feedback", "")
            )
            rounds.append(round_data)
        
        # è®¡ç®—å„é¡¹æŒ‡æ ‡
        ea_score, ea_details = self.ea_evaluator.calculate_alignment_score(rounds)
        as_score, as_details = self.as_evaluator.calculate_similarity_score(rounds)
        sr_score, sr_details = self.sr_evaluator.calculate_rationality_score(rounds)
        om_score, om_details = self.om_evaluator.calculate_outcome_score(rounds)
        
        # è®¡ç®—æœ€ç»ˆå¾—åˆ†
        final_score = (
            self.weights["ea"] * ea_score +
            self.weights["as"] * as_score + 
            self.weights["sr"] * sr_score +
            self.weights["om"] * om_score
        )
        
        # ç»„è£…è¯¦ç»†æŒ‡æ ‡
        detailed_metrics = {
            "ea_details": ea_details,
            "as_details": as_details, 
            "sr_details": sr_details,
            "om_details": om_details,
            "weights": self.weights,
            "evaluation_timestamp": datetime.now().isoformat()
        }
        
        return EvaluationResult(
            ea_score=ea_score,
            as_score=as_score,
            sr_score=sr_score,
            om_score=om_score,
            final_score=final_score,
            detailed_metrics=detailed_metrics
        )
    
    def export_results_to_json(self, result: EvaluationResult, filepath: str):
        """å¯¼å‡ºè¯„æµ‹ç»“æœåˆ°JSONæ–‡ä»¶"""
        
        def convert_numpy_types(obj):
            """é€’å½’è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹"""
            if hasattr(obj, 'item'):  # numpyæ ‡é‡
                return obj.item()
            elif hasattr(obj, 'tolist'):  # numpyæ•°ç»„
                return obj.tolist()
            elif isinstance(obj, dict):
                return {k: convert_numpy_types(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy_types(item) for item in obj]
            else:
                return obj
        
        result_dict = {
            "scores": {
                "ea_score": convert_numpy_types(result.ea_score),
                "as_score": convert_numpy_types(result.as_score), 
                "sr_score": convert_numpy_types(result.sr_score),
                "om_score": convert_numpy_types(result.om_score),
                "final_score": convert_numpy_types(result.final_score)
            },
            "detailed_metrics": convert_numpy_types(result.detailed_metrics)
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(result_dict, f, ensure_ascii=False, indent=2)


class DataExporter:
    """æ•°æ®å¯¼å‡ºå™¨ï¼Œå°†ç°æœ‰æ—¥å¿—è½¬æ¢ä¸ºè¯„æµ‹æ ¼å¼"""
    
    @staticmethod
    def convert_experiment_logs_to_evaluation_format(experiment_dir: str) -> List[Dict[str, Any]]:
        """
        å°†å®éªŒæ—¥å¿—è½¬æ¢ä¸ºè¯„æµ‹æ ¼å¼
        
        Args:
            experiment_dir: å®éªŒç›®å½•è·¯å¾„
            
        Returns:
            è¯„æµ‹æ ¼å¼çš„æ•°æ®åˆ—è¡¨
        """
        # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„æ—¥å¿—æ ¼å¼è¿›è¡Œè°ƒæ•´
        # æš‚æ—¶æä¾›ä¸€ä¸ªæ¨¡æ¿å®ç°
        
        evaluation_data = []
        
        # å°è¯•ä»ä¸åŒçš„æ—¥å¿—æ–‡ä»¶ä¸­è¯»å–æ•°æ®
        exp_path = Path(experiment_dir)
        
        # æŸ¥æ‰¾å†³ç­–æ—¥å¿—æ–‡ä»¶
        decisions_dir = exp_path / "decisions"
        if decisions_dir.exists():
            for json_file in decisions_dir.glob("*.json"):
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        # æ ¹æ®å®é™…æ•°æ®ç»“æ„è¿›è¡Œè½¬æ¢
                        # è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“çš„æ—¥å¿—æ ¼å¼è¿›è¡Œè°ƒæ•´
                        pass
                except Exception as e:
                    print(f"è¯»å–å†³ç­–æ–‡ä»¶å¤±è´¥ {json_file}: {e}")
        
        return evaluation_data
    
    @staticmethod  
    def convert_game_logs_to_evaluation_format(log_dir: str) -> List[Dict[str, Any]]:
        """
        å°†æ¸¸æˆæ—¥å¿—è½¬æ¢ä¸ºè¯„æµ‹æ ¼å¼
        
        Args:
            log_dir: æ¸¸æˆæ—¥å¿—ç›®å½•
            
        Returns:
            è¯„æµ‹æ ¼å¼çš„æ•°æ®åˆ—è¡¨
        """
        evaluation_data = []
        
        # å®ç°ä»CSVæˆ–å…¶ä»–æ ¼å¼è½¬æ¢çš„é€»è¾‘
        # è¿™é‡Œæä¾›ä¸€ä¸ªåŸºç¡€æ¨¡æ¿
        
        return evaluation_data


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    def load_historical_reference_data(json_file_path: str) -> List[Dict[str, Any]]:
        """ä»JSONæ–‡ä»¶åŠ è½½å†å²å‚è€ƒæ•°æ®"""
        try:
            with open(json_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                print(f"æˆåŠŸåŠ è½½å†å²å‚è€ƒæ•°æ®ï¼Œå…± {len(data)} æ¡è®°å½•")
                return data
        except FileNotFoundError:
            print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {json_file_path}")
            return []
        except json.JSONDecodeError as e:
            print(f"é”™è¯¯ï¼šJSONæ–‡ä»¶æ ¼å¼é”™è¯¯ - {e}")
            return []
        except Exception as e:
            print(f"é”™è¯¯ï¼šè¯»å–æ–‡ä»¶æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ - {e}")
            return []
    
    # ä»JSONæ–‡ä»¶åŠ è½½çœŸå®å¤å·´å¯¼å¼¹å±æœºå†å²äº‹ä»¶æ•°æ®
    historical_reference_data = load_historical_reference_data("experiments/unified_comparison_0829_1455/werewolf_test_0829_1509/evaluation/evaluation_data.json")
    
    # åˆ›å»ºè¯„æµ‹ç³»ç»Ÿ
    evaluator = ModelEvaluationSystem()
    
    # è¿›è¡Œè¯„æµ‹
    result = evaluator.evaluate(historical_reference_data)
    
    # è¾“å‡ºç»“æœ
    print("=== åŸºäºçœŸå®å¤å·´å¯¼å¼¹å±æœºå†å²æ•°æ®çš„è¯„æµ‹ç»“æœ ===")
    print(f"å†å²äº‹ä»¶å¯¹é½åº¦ (EA): {result.ea_score:.3f}")
    print(f"è¡ŒåŠ¨å†…å®¹ç›¸ä¼¼åº¦ (AS): {result.as_score:.3f}")
    print(f"æˆ˜ç•¥åˆç†æ€§ (SR): {result.sr_score:.3f}")
    print(f"ç»“æœä¸€è‡´æ€§ (OM): {result.om_score:.3f}")
    print(f"æœ€ç»ˆå¾—åˆ† (FS): {result.final_score:.3f}")
    
    # è¾“å‡ºè¯¦ç»†åˆ†æ
    print("\n=== è¯¦ç»†åˆ†æ ===")
    print("äº‹ä»¶å¯¹é½è¯¦æƒ…:")
    for match in result.detailed_metrics["ea_details"]["event_matches"]:
        if match["matched"]:
            print(f"  âœ“ {match['historical']['type']} -> åŒ¹é…åˆ°ç¬¬{match['simulation']['round']}è½®")
        else:
            print(f"  âœ— {match['historical']['type']} -> æœªåŒ¹é…")
    
    print(f"\næˆ˜ç•¥åˆç†æ€§æ£€æŸ¥:")
    violations = result.detailed_metrics["sr_details"]["violations"]
    if violations:
        for v in violations:
            print(f"  âš  ç¬¬{v['round']}è½®: {v['rule_name']} - {v['reason']}")
    else:
        print("  âœ“ æœªå‘ç°ä¸åˆç†è¡Œä¸º")
    
    print(f"\nç»“æœä¸€è‡´æ€§å¯¹æ¯”:")
    outcome_details = result.detailed_metrics["om_details"]["detailed_comparison"]
    for criterion, details in outcome_details.items():
        status = "âœ“" if details["match"] else "âœ—"
        print(f"  {status} {criterion}: å†å²={details['historical']}, ä»¿çœŸ={details['simulation']}")