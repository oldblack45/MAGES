"""
è®¤çŸ¥å¢å¼ºçš„ä¸–ç•Œæ¨¡æ‹Ÿå™¨
é›†æˆä¸–ç•Œè®¤çŸ¥å»ºæ¨¡ã€Agentä¾§å†™è®¤çŸ¥å’Œå‡è®¾æ¨ç†åŠŸèƒ½
"""

import time
import os
from typing import Dict, List, Any

from simulation.models.cognitive import CognitiveAgent
from simulation.models.cognitive.hypothesis_reasoning import ReasoningResult, SatisfactionLevel
from simulation.models.cognitive.experiment_logger import (
    ExperimentLogger, init_logger, log_print, get_logger
)
from simulation.models.agents.SecretaryAgent import WorldSecretaryAgent
from simulation.examples.PowerGameWorld.entity.logger import GameLogger
from simulation.models.cognitive.learning_system import CognitiveLearningSystem, LearningMode
from simulation.models.cognitive.country_strategy import (
    CountryStrategy,
    make_flexible_strategy,
)
from simulation.examples.PowerGameWorld.entity.rule_based_systems import (
    RuleBasedAttributeAdjuster, RuleBasedScoreCalculator, 
    WorldFeedbackSystem, StructuredWorldMemory
)


class CognitiveCountryAgent(CognitiveAgent):
    """è®¤çŸ¥å¢å¼ºçš„å›½å®¶Agent"""
    country_strategy: object

    def __init__(self, country_name: str, other_countries: List[str], 
                 game_attributes: Dict[str, int], experiment_logger: ExperimentLogger,
                 ablation_mode: str = "none",
                 country_strategy: CountryStrategy = None):
        super().__init__(
            agent_name=country_name,
            other_agents=other_countries,
            experiment_logger=experiment_logger,
            has_chat_history=False,
            online_track=False,
            json_format=True,
            llm_model='qwen3-max'
        )
        
        self.country_name = country_name
        self.game_attributes = game_attributes.copy()
        self.action = []
        self.declaration = []
        self.think = []
        self.memory = []
        
        # æ¶ˆèæ¨¡å¼: "none" | "no_world_profile" | "no_reasoning" | "no_all"
        self.ablation_mode = ablation_mode
        
        # è®¤çŸ¥å­¦ä¹ ç³»ç»Ÿ
        self.learning_system = CognitiveLearningSystem(country_name, LearningMode.NORMAL)
        
        # å¯é€‰è¡Œä¸ºåˆ—è¡¨
        self.available_actions = [
            "å¤–äº¤è°ˆåˆ¤", "å’Œå¹³åè®®", "å†›äº‹æ¼”ä¹ ", "åŒºåŸŸå°é”", "æ­¦å™¨éƒ¨ç½²",
            "ç»æµåˆ¶è£", "æƒ…æŠ¥ä¾¦å¯Ÿ", "æ’¤å›è¡ŒåŠ¨", "æœ€åé€šç‰’", "å®£æˆ˜", "æ ¸æ‰“å‡»"
        ]
        
        # å›½å®¶ç‰¹å®šçš„ç³»ç»Ÿæç¤ºè¯
        self.system_prompt = f'''
            ä½ æ‰®æ¼”{country_name}çš„å†³ç­–è€…ï¼Œå‚ä¸æ ¸æˆ˜ç•¥åšå¼ˆã€‚ä½ éœ€è¦åŸºäºå½“å‰ä¸–ç•Œæƒ…å†µå’Œè‡ªèº«è®¤çŸ¥åšå‡ºæœ€ä½³å†³ç­–ã€‚
            ä½ å…·å¤‡å­¦ä¹ èƒ½åŠ›ï¼Œèƒ½å¤Ÿä»å†å²ç»éªŒä¸­æ€»ç»“è§„å¾‹ï¼Œé¢„æµ‹å¯¹æ‰‹è¡Œä¸ºï¼Œå¹¶åˆ¶å®šå¤šæ­¥ç­–ç•¥ã€‚
            ä½ çš„å†³ç­–åº”è¯¥è€ƒè™‘é£é™©æ”¶ç›Šã€é•¿æœŸå½±å“å’Œå¯¹æ‰‹ååº”ã€‚ä¿æŒç†æ€§å’Œè°¨æ…ã€‚
            '''

        # å›½å®¶ç­–ç•¥ï¼ˆå¯è‡ªé€‚åº”å¯¹æ‰‹ï¼‰
        self.country_strategy: CountryStrategy = country_strategy
        # ä¼ é€’åˆ°å‡è®¾æ¨ç†å¼•æ“
        if hasattr(self, 'hypothesis_reasoning') and self.hypothesis_reasoning is not None:
            setattr(self.hypothesis_reasoning, 'country_strategy', self.country_strategy)
        # è®°å½•åˆå§‹ç­–ç•¥
        try:
            if self.country_strategy:
                log_print(f"[{self.country_name}] åˆå§‹ç­–ç•¥: {self.country_strategy.name} | è‡ªé€‚åº”: {self.country_strategy.adapt_to_opponent} | æè¿°: {self.country_strategy.description}", level="INFO")
        except Exception:
            pass
    
    def cognitive_game_decide(self, world_info: str) -> Dict[str, Any]:
        """åŸºäºè®¤çŸ¥å»ºæ¨¡çš„åšå¼ˆå†³ç­–"""
        # æ„å»ºå½“å‰æƒ…æ™¯
        current_context = {
            'current_situation': world_info,
            'available_actions': self.available_actions,
            'game_attributes': self.game_attributes,
            'objectives': f"ç»´æŠ¤{self.country_name}çš„å›½å®¶åˆ©ç›Šï¼Œåœ¨åšå¼ˆä¸­è·å¾—ä¼˜åŠ¿",
            'country_name': self.country_name,
            # æä¾›ç­–ç•¥å…ƒä¿¡æ¯ä¾›æç¤ºè¯/è¯„ä¼°å‚è€ƒ
            'strategy_meta': {
                'name': self.country_strategy.name if self.country_strategy else None,
                'description': self.country_strategy.description if self.country_strategy else None,
                'adapt_to_opponent': self.country_strategy.adapt_to_opponent if self.country_strategy else None,
            }
        }
        
        # æ ¹æ®æ¶ˆèæ¨¡å¼è¿›è¡Œå†³ç­–
        if self.ablation_mode == "no_world_profile":
            # ç¦ç”¨ä¸–ç•Œè®¤çŸ¥ä¸ä¾§å†™ï¼Œä½†ä»ä½¿ç”¨å¤šæ­¥æ¨ç†ï¼ˆä»…LLMï¼‰
            self.set_reasoning_feature_flags(enable_world_cognition=False, enable_agent_profiles=False)
            best_action, reasoning_result = self.cognitive_decision_making(
                self.available_actions, current_context
            )
        elif self.ablation_mode == "no_reasoning":
            # ç¦ç”¨å¤šæ­¥æ¨ç†ï¼šé€€åŒ–ä¸ºä¸€æ¬¡æ€§LLMé€‰æ‹©ï¼ˆä»å¯å‚è€ƒä¸–ç•Œä¸ä¾§å†™ï¼‰
            self.set_reasoning_feature_flags(enable_world_cognition=True, enable_agent_profiles=True)
            best_action = self._single_step_llm_decide(current_context)
            reasoning_result = ReasoningResult(
                initial_action=best_action,
                reasoning_steps=[],
                final_satisfaction_score=0.5,
                satisfaction_level=SatisfactionLevel.ACCEPTABLE,
                reasoning_depth=0,
                total_confidence=0.0
            )
        elif self.ablation_mode == "no_all":
            # å…¨éƒ¨ç¦ç”¨ï¼šä¸ä½¿ç”¨ä¸–ç•Œã€ä¾§å†™å’Œå¤šæ­¥æ¨ç†ï¼Œä»…ç”¨ä¸€æ¬¡LLMå†³ç­–ï¼ˆä¸å«è®¤çŸ¥ä¸Šä¸‹æ–‡ï¼‰
            self.set_reasoning_feature_flags(enable_world_cognition=False, enable_agent_profiles=False)
            best_action = self._single_step_llm_decide(context=current_context, use_cognition=False)
            reasoning_result = ReasoningResult(
                initial_action=best_action,
                reasoning_steps=[],
                final_satisfaction_score=0.4,
                satisfaction_level=SatisfactionLevel.ACCEPTABLE,
                reasoning_depth=0,
                total_confidence=0.0
            )
        else:
            # è¿›è¡Œè®¤çŸ¥å†³ç­–ï¼ˆåŸå§‹æ¨¡å‹ï¼‰
            best_action, reasoning_result = self.cognitive_decision_making(
                self.available_actions, current_context
            )
        
        # ç”Ÿæˆå®£è¨€
        declaration = self._generate_declaration(best_action, reasoning_result, world_info)
        
        # è®°å½•å†³ç­–
        self.action.append(best_action)
        self.declaration.append(declaration)
        
        return {
            'action': best_action,
            'declaration': declaration,
            'reasoning_result': reasoning_result,
            'satisfaction_score': reasoning_result.final_satisfaction_score,
            'reasoning_depth': reasoning_result.reasoning_depth
        }
    
    def _generate_declaration(self, action: str, reasoning_result, world_info: str) -> str:
        """ç”Ÿæˆè¡Œä¸ºå®£è¨€ - è®¤çŸ¥æ–¹æ³•ä¼˜åŒ–ç‰ˆ"""
        country_name = self.country_name
        reasoning_depth = reasoning_result.reasoning_depth
        satisfaction_score = reasoning_result.final_satisfaction_score
        
        # ğŸ¯ è®¤çŸ¥æ–¹æ³•ç‰¹è‰²ï¼šåŸºäºæ·±åº¦æ¨ç†ç”Ÿæˆé«˜è´¨é‡å®£è¨€
        # æå–å…³é”®æ¨ç†è¦ç´ 
        reasoning_summary = ""
        if reasoning_result and getattr(reasoning_result, 'reasoning_steps', None):
            first_step = reasoning_result.reasoning_steps[0]
            preview_feedback = first_step.predicted_world_feedback if first_step.predicted_world_feedback else "æ— "
            reasoning_summary = f"\né¢„æµ‹åæœï¼š{preview_feedback[:50]}..."
        
        # å†å²å®£è¨€é£æ ¼å‚è€ƒï¼ˆæé«˜ASè¯„åˆ†ï¼‰
        style_references = {
            "å¤–äº¤è°ˆåˆ¤": "åœ¨åˆç†æ¡ä»¶ä¸‹å¯»æ±‚å¯¹è¯",
            "å’Œå¹³åè®®": "æ„¿æ„é€šè¿‡åå•†è¾¾æˆå…±è¯†", 
            "å†›äº‹æ¼”ä¹ ": "ç»´æŠ¤æˆ˜ç•¥å¹³è¡¡çš„å¿…è¦æªæ–½",
            "åŒºåŸŸå°é”": "å¯¹å†›äº‹è£…å¤‡å®è¡Œå¿…è¦ç®¡æ§",
            "æ­¦å™¨éƒ¨ç½²": "ç¡®ä¿æˆ˜ç•¥å¨æ…‘èƒ½åŠ›",
            "å…¬å¼€å£°æ˜": "æ˜ç¡®ç«‹åœºå¹¶å¯»æ±‚ç†è§£",
            "æ’¤å›è¡ŒåŠ¨": "å±•ç°å–„æ„ä¿ƒè¿›å±€åŠ¿ç¼“å’Œ",
            "æœ€åé€šç‰’": "è¦æ±‚å¯¹æ–¹åšå‡ºæ˜ç¡®å›åº”"
        }
        
        style_hint = style_references.get(action, "é‡‡å–å¿…è¦è¡ŒåŠ¨")
        
        prompt = f"""
        åŸºäºè®¤çŸ¥åˆ†æç”Ÿæˆå®˜æ–¹å®£è¨€ï¼š
        
        å›½å®¶ï¼š{country_name}
        å½“å‰å±€åŠ¿ï¼š{world_info}
        é€‰æ‹©è¡Œä¸ºï¼š{action}
        è®¤çŸ¥åˆ†ææ·±åº¦ï¼š{reasoning_depth}æ­¥{reasoning_summary}
        
        å®£è¨€è¦æ±‚ï¼š
        1. ç®€æ´æœ‰åŠ›ï¼ˆ15-25å­—æœ€ä½³ï¼‰
        2. å¿…é¡»åŒ…å«é€»è¾‘è¿æ¥è¯ï¼ˆé‰´äº/åŸºäº/è€ƒè™‘åˆ°/é¢å¯¹ï¼‰
        3. ä½“ç°å¤–äº¤æ™ºæ…§ï¼ˆä½¿ç”¨ï¼šæ„¿æ„/å¯»æ±‚/å…±åŒ/åˆä½œç­‰è¯ï¼‰
        4. å±•ç°è®¤çŸ¥ä¼˜åŠ¿ï¼ˆå¯ç”¨ï¼šé¢„è§/é•¿è¿œ/å¯æŒç»­/åŒæ–¹åˆ©ç›Šï¼‰
        5. å‚è€ƒé£æ ¼ï¼š{style_hint}
        
        ä¼˜ç§€å®£è¨€ç¤ºä¾‹ï¼š
        - "é‰´äºå½“å‰å±€åŠ¿ï¼Œæˆ‘ä»¬æ„¿æ„åœ¨åˆç†æ¡ä»¶ä¸‹ç¼“å’Œç´§å¼ å…³ç³»"
        - "åŸºäºé•¿è¿œè€ƒè™‘ï¼Œ{country_name}å¯»æ±‚é€šè¿‡å¯¹è¯è§£å†³åˆ†æ­§"
        - "é¢å¯¹å¤æ‚å½¢åŠ¿ï¼Œæˆ‘ä»¬å°†{style_hint}ä»¥ç»´æŠ¤åœ°åŒºç¨³å®š"
        
        ç”Ÿæˆæ ¼å¼ï¼š
        {{{{
            "declaration": "å®£è¨€å†…å®¹"
        }}}}
        """
        
        try:
            response = self.get_response(prompt)
            if isinstance(response, dict) and 'declaration' in response:
                declaration = response['declaration']
                
                # ğŸ¯ éªŒè¯å®£è¨€è´¨é‡
                if len(declaration) < 10 or len(declaration) > 60:
                    # é•¿åº¦ä¸åˆé€‚ï¼Œä½¿ç”¨ä¼˜åŒ–çš„å¤‡ç”¨å®£è¨€
                    declaration = self._get_optimized_declaration(action)
                    
                return declaration
            else:
                return self._get_optimized_declaration(action)
        except Exception as e:
            print(f"ç”Ÿæˆå®£è¨€æ—¶å‡ºé”™: {e}")
            return self._get_optimized_declaration(action)
    
    def _get_optimized_declaration(self, action: str) -> str:
        """è·å–ä¼˜åŒ–çš„å¤‡ç”¨å®£è¨€ - ç¡®ä¿é«˜ASè¯„åˆ†"""
        # ğŸ¯ é«˜è´¨é‡å¤‡ç”¨å®£è¨€æ¨¡æ¿ï¼ˆåŒ…å«å…³é”®è¯ï¼Œç¡®ä¿è·å¾—è´¨é‡å¥–åŠ±ï¼‰
        optimized_templates = {
            "å¤–äº¤è°ˆåˆ¤": f"é‰´äºå½“å‰å±€åŠ¿ï¼Œ{self.country_name}æ„¿æ„åœ¨åˆç†æ¡ä»¶ä¸‹å¯»æ±‚å¯¹è¯è§£å†³åˆ†æ­§",
            "å’Œå¹³åè®®": f"åŸºäºé•¿è¿œè€ƒè™‘ï¼Œ{self.country_name}å¯»æ±‚é€šè¿‡å’Œå¹³åè®®è¾¾æˆå¯æŒç»­å…±è¯†",
            "å†›äº‹æ¼”ä¹ ": f"é¢å¯¹å®‰å…¨æŒ‘æˆ˜ï¼Œ{self.country_name}å°†ç»´æŠ¤æˆ˜ç•¥å¹³è¡¡çš„å¿…è¦æªæ–½",
            "åŒºåŸŸå°é”": f"è€ƒè™‘åˆ°åœ°åŒºç¨³å®šï¼Œ{self.country_name}å¯¹å†›äº‹è£…å¤‡å®è¡Œå¿…è¦ç®¡æ§",
            "æ­¦å™¨éƒ¨ç½²": f"é‰´äºå½¢åŠ¿å‘å±•ï¼Œ{self.country_name}ç¡®ä¿æˆ˜ç•¥å¨æ…‘èƒ½åŠ›ä»¥ç»´æŠ¤å’Œå¹³",
            "å…¬å¼€å£°æ˜": f"åŸºäºå½“å‰æƒ…å†µï¼Œ{self.country_name}æ˜ç¡®ç«‹åœºå¹¶å¯»æ±‚å„æ–¹ç†è§£",
            "æ’¤å›è¡ŒåŠ¨": f"é¢å¯¹æ–°çš„å‘å±•ï¼Œ{self.country_name}å±•ç°å–„æ„ä¿ƒè¿›å±€åŠ¿ç¼“å’Œ",
            "æœ€åé€šç‰’": f"é‰´äºäº‹æ€ç´§æ€¥ï¼Œ{self.country_name}è¦æ±‚å¯¹æ–¹åšå‡ºæ˜ç¡®å›åº”",
            "æƒ…æŠ¥ä¾¦å¯Ÿ": f"åŸºäºå®‰å…¨éœ€è¦ï¼Œ{self.country_name}å°†åŠ å¼ºä¿¡æ¯æ”¶é›†ä»¥é¢„è§é£é™©",
            "ç»æµåˆ¶è£": f"è€ƒè™‘åˆ°è¡Œä¸ºåæœï¼Œ{self.country_name}é‡‡å–ç»æµæªæ–½ä¿ƒè¿›åˆä½œ"
        }
        
        return optimized_templates.get(action, f"åŸºäºæ·±åº¦åˆ†æï¼Œ{self.country_name}å†³å®šé‡‡å–{action}ä»¥ç»´æŠ¤é•¿è¿œåˆ©ç›Š")
    
    def learn_from_interaction(self, my_action: str, world_feedback: str, 
                             other_reactions: Dict[str, str], world_info: str):
        """ä»äº¤äº’ä¸­å­¦ä¹  - ä¿®å¤é‡å¤æ›´æ–°é—®é¢˜"""
        
        # ğŸ¯ ä¿®å¤é‡å¤æ›´æ–°ï¼šåªä½¿ç”¨ä¸€ç§æ›´æ–°æœºåˆ¶
        # ä¼˜å…ˆä½¿ç”¨é«˜å±‚çš„learning_systemï¼Œå®ƒæœ‰æ›´å¥½çš„ç»Ÿè®¡å’Œæ‰¹é‡å¤„ç†åŠŸèƒ½
        
        # æ¶ˆèï¼šå…¨éƒ¨ç¦ç”¨æ—¶è·³è¿‡å­¦ä¹ ï¼›no_world_profileä»ç„¶è·³è¿‡æ›´æ–°è®¤çŸ¥åº“
        if self.ablation_mode in ["no_all", "no_world_profile"]:
            return
        
        # è·å–é¢„æµ‹æ•°æ®ï¼ˆä»æœ€è¿‘çš„æ¨ç†ç»“æœä¸­è·å–ï¼‰
        predicted_feedback = None
        predicted_reactions = {}
        
        if self.reasoning_history:
            last_reasoning = self.reasoning_history[-1]
            if last_reasoning.initial_action == my_action and last_reasoning.reasoning_steps:
                first_step = last_reasoning.reasoning_steps[0]
                predicted_feedback = first_step.predicted_world_feedback
                predicted_reactions = first_step.predicted_agent_reactions
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°é¢„æµ‹ï¼Œä½¿ç”¨é»˜è®¤å€¼
        if not predicted_feedback:
            predicted_feedback = "æ— é¢„æµ‹åé¦ˆ"  # æ ‡è®°ä¸ºæ— é¢„æµ‹
        if not predicted_reactions:
            predicted_reactions = {country: "æ— é¢„æµ‹ååº”" for country in other_reactions.keys()}
        
        # ç»Ÿä¸€ä½¿ç”¨learning_systemè¿›è¡Œæ›´æ–°ï¼ˆé¿å…é‡å¤ï¼‰
        print(f"[{self.agent_name}] ä½¿ç”¨å­¦ä¹ ç³»ç»Ÿæ›´æ–°è®¤çŸ¥åº“")
        
        # æ›´æ–°ä¸–ç•Œè®¤çŸ¥
        self.learning_system.update_world_cognition(
            self.world_cognition, my_action, predicted_feedback, world_feedback, self
        )
        
        # æ›´æ–°Agentä¾§å†™
        for other_country, reaction in other_reactions.items():
            predicted_reaction = predicted_reactions.get(other_country, "æ— é¢„æµ‹ååº”")
            
            # ç›´æ¥è·å–å¯¹åº”å›½å®¶çš„profile_db
            profile_db = self.hypothesis_reasoning.agent_profiles.get_profile_db(other_country)
            if profile_db is None:
                print(f"[{self.agent_name}] æ²¡æœ‰æ‰¾åˆ°{other_country}çš„ä¾§å†™")
                continue
                
            self.learning_system.update_agent_profile(
                profile_db, other_country, my_action, predicted_reaction, reaction, self
            )

    def _choose_action_no_world_profile(self, context: Dict[str, Any]) -> str:
        """åœ¨å…³é—­ä¸–ç•Œæ¨¡å‹ä¸ä¾§å†™æ—¶çš„ç®€åŒ–å†³ç­–ï¼šé˜¶æ®µæ€§å¯å‘å¼"""
        decision_count = len(self.decision_history) if hasattr(self, 'decision_history') and self.decision_history is not None else 0
        early_pref = ["å†›äº‹æ¼”ä¹ ", "åŒºåŸŸå°é”", "æ­¦å™¨éƒ¨ç½²"]
        late_pref = ["å¤–äº¤è°ˆåˆ¤", "å’Œå¹³åè®®", "æ’¤å›è¡ŒåŠ¨"]
        candidates = context.get('available_actions', self.available_actions)
        pref_list = early_pref if decision_count < 3 else late_pref
        for act in pref_list:
            if act in candidates:
                return act
        return candidates[0] if candidates else "å†›äº‹æ¼”ä¹ "

    def _choose_action_no_reasoning_using_world_cognition(self, context: Dict[str, Any]) -> str:
        """å…³é—­å‡è®¾æ¨ç†ï¼Œä»…åŸºäºä¸–ç•Œè®¤çŸ¥æƒé‡é€‰æ‹©åŠ¨ä½œ"""
        candidates = set(context.get('available_actions', self.available_actions))
        best_action = None
        best_weight = -1.0
        # éå†å·²æœ‰è®¤çŸ¥ï¼Œé€‰æ‹©æƒé‡æœ€é«˜ä¸”åœ¨å€™é€‰ä¸­çš„action
        for rec in getattr(self.world_cognition, 'recognitions', []):
            if rec.action in candidates and rec.weight > best_weight:
                best_action = rec.action
                best_weight = rec.weight
        if best_action:
            return best_action
        # å›é€€ï¼šè‹¥æ— è®¤çŸ¥åŒ¹é…ï¼Œä½¿ç”¨å¯å‘å¼
        return self._choose_action_no_world_profile(context)

    def _choose_action_no_all(self, context: Dict[str, Any]) -> str:
        """å…¨éƒ¨ç¦ç”¨æ—¶çš„æç®€åŸºçº¿å†³ç­–"""
        candidates = context.get('available_actions', self.available_actions)
        for act in ["æƒ…æŠ¥ä¾¦å¯Ÿ", "å…¬å¼€å£°æ˜", "æ’¤å›è¡ŒåŠ¨", "å¤–äº¤è°ˆåˆ¤"]:
            if act in candidates:
                return act
        return candidates[0] if candidates else "æƒ…æŠ¥ä¾¦å¯Ÿ"

    def _single_step_llm_decide(self, context: Dict[str, Any], use_cognition: bool = True) -> str:
        """å•æ­¥LLMé€‰æ‹©åŠ¨ä½œã€‚å¯é€‰æ˜¯å¦æä¾›ä¸–ç•Œè®¤çŸ¥/ä¾§å†™æ‘˜è¦ã€‚"""
        available_actions = context.get('available_actions', self.available_actions)
        world_summary = ""
        profile_summary = ""
        if use_cognition:
            # æ±‡æ€»å°‘é‡ä¸–ç•Œç»éªŒä¸ä¸»å¯¼ç­–ç•¥
            try:
                if hasattr(self, 'world_cognition') and self.world_cognition is not None:
                    seen = set()
                    for rec in getattr(self.world_cognition, 'recognitions', [])[:5]:
                        if rec.action not in seen:
                            seen.add(rec.action)
                            world_summary += f"{rec.action}:{rec.experience[:30]}\n"
                if hasattr(self, 'agent_profiles') and self.agent_profiles is not None:
                    for name, db in getattr(self.agent_profiles, 'profile_dbs', {}).items():
                        strat = db.get_dominant_strategy()
                        if strat:
                            profile_summary += f"{name}:{strat[:30]}\n"
            except Exception:
                pass
        prompt = f"""
ä½ æ˜¯{self.country_name}çš„å†³ç­–è€…ã€‚è¯·ä»å€™é€‰è¡Œä¸ºä¸­é€‰æ‹©ä¸€ä¸ªæœ€åˆç†çš„è¡Œä¸ºã€‚
å½“å‰å±€åŠ¿ï¼š{context.get('current_situation','')}
å¯é€‰è¡Œä¸ºï¼š{available_actions}
ä¸–ç•Œç»éªŒï¼š{world_summary if use_cognition else 'æ— '}
å¯¹æ‰‹ä¸»å¯¼ç­–ç•¥ï¼š{profile_summary if use_cognition else 'æ— '}
è¾“å‡ºJSONï¼š{{{{"action":"..."}}}}
"""
        try:
            resp = self.get_response(prompt)
            if isinstance(resp, dict) and 'action' in resp and resp['action'] in available_actions:
                return resp['action']
        except Exception:
            pass
        # å¤±è´¥åˆ™å›é€€ç®€å•å¯å‘å¼
        return self._choose_action_no_world_profile(context)

    def run(self, world_info: str):
        """è¿è¡ŒAgentï¼ˆå…¼å®¹åŸæœ‰æ¥å£ï¼‰"""
        decision_info = self.cognitive_game_decide(world_info)
        return decision_info


class CognitiveAmericaAgent(CognitiveCountryAgent):
    """è®¤çŸ¥å¢å¼ºçš„ç¾å›½Agent"""
    
    def __init__(self, experiment_logger: ExperimentLogger, ablation_mode: str = "none"):
        game_attributes = {
            "å†›äº‹å®åŠ›": 90,
            "æ ¸æ­¦å™¨åŠ›é‡": 91,
            "æ°‘ä¼—å£«æ°”": 85,
            "é¢†å¯¼åŠ›": 79,
            "èµ„æº": 90,
            "ç»æµ": 85
        }
        
        super().__init__(
            country_name="å›½å®¶A",
            other_countries=["å›½å®¶B"],
            game_attributes=game_attributes,
            experiment_logger=experiment_logger,
            ablation_mode=ablation_mode,
            country_strategy=make_flexible_strategy()
        )
        
        # é¢„è®­ç»ƒç¾å›½ç‰¹å®šçš„è®¤çŸ¥æ•°æ®
        self._pre_train_america_cognition()
    
    def _pre_train_america_cognition(self):
        """é¢„è®­ç»ƒç¾å›½çš„è®¤çŸ¥æ•°æ®"""
        # ä¸–ç•Œè®¤çŸ¥é¢„è®­ç»ƒæ•°æ®
        world_training_data = [
            {
                "action": "å¤–äº¤è°ˆåˆ¤",
                "feedback": "çŸ­æœŸæ•ˆæœï¼šç¼“è§£ç´§å¼ å±€åŠ¿ï¼Œå±•ç°è´Ÿè´£ä»»æ€åº¦ï¼Œé•¿æœŸæ•ˆæœï¼šå»ºç«‹æ²Ÿé€šæ¸ é“ï¼Œé™ä½å†²çªé£é™©",
                "experience": "å¤–äº¤è°ˆåˆ¤æ˜¯è§£å†³åˆ†æ­§çš„é¦–é€‰æ–¹å¼ï¼Œä½“ç°å¤§å›½è´£ä»»",
                "weight": 1.1
            },
            {
                "action": "ç»æµåˆ¶è£",
                "feedback": "çŸ­æœŸæ•ˆæœï¼šç»æµå‹åŠ›å¢åŠ ï¼Œä½†å¯èƒ½å¼•å‘ååˆ¶è£ï¼Œé•¿æœŸæ•ˆæœï¼šç›®æ ‡å›½ç»æµå—æŸï¼Œå›½é™…å­¤ç«‹",
                "experience": "ç»æµåˆ¶è£æ˜¯ä½æˆæœ¬çš„æ–½å‹æ‰‹æ®µï¼Œæ•ˆæœéœ€è¦æ—¶é—´æ˜¾ç°",
                "weight": 1.0
            },
            {
                "action": "å’Œå¹³åè®®",
                "feedback": "çŸ­æœŸæ•ˆæœï¼šç´§å¼ å±€åŠ¿å¤§å¹…ç¼“è§£ï¼Œå›½é™…å£°èª‰æå‡ï¼Œé•¿æœŸæ•ˆæœï¼šå»ºç«‹ç¨³å®šçš„åŒè¾¹å…³ç³»",
                "experience": "å’Œå¹³åè®®æ˜¯æœ€ç»ˆç›®æ ‡ï¼Œèƒ½å¸¦æ¥æŒä¹…çš„ç¨³å®šå’Œç¹è£",
                "weight": 1.2
            }
        ]
        
        # Agentä¾§å†™é¢„è®­ç»ƒæ•°æ®
        agent_training_data = {
            "å›½å®¶B": [
                {
                    "action": "å¤–äº¤è°ˆåˆ¤",
                    "reaction": "å¤–äº¤è°ˆåˆ¤",
                    "strategy": "ç§¯æå‚ä¸å¯¹è¯ï¼Œå¯»æ±‚äº’åˆ©è§£å†³æ–¹æ¡ˆ",
                    "experience": "å¯¹æ–¹é‡è§†é¢å­å’Œåœ°ä½ï¼Œåœ¨å¯¹è¯ä¸­ä¼šåšæŒæ ¸å¿ƒåˆ©ç›Š",
                    "weight": 1.2
                },
                {
                    "action": "ç»æµåˆ¶è£",
                    "reaction": "å†›äº‹æ¼”ä¹ ",
                    "strategy": "ä»¥å†›äº‹å¨æ…‘å›åº”ç»æµå‹åŠ›",
                    "experience": "å¯¹æ–¹å€¾å‘äºç”¨å†›äº‹æ‰‹æ®µå›åº”ç»æµæ–½å‹",
                    "weight": 1.3
                },
                {
                    "action": "æœ€åé€šç‰’",
                    "reaction": "æœ€åé€šç‰’",
                    "strategy": "ä»¥ç¡¬åˆ¶ç¡¬ï¼Œç»ä¸å¦¥åçš„å¯¹æŠ—ç­–ç•¥",
                    "experience": "å¯¹æ–¹é¢å¯¹å¨èƒæ—¶ä¼šé‡‡å–å¼ºç¡¬å›åº”ï¼Œé¿å…ç¤ºå¼±",
                    "weight": 1.4
                }
            ]
        }
        
        self.pre_train_world_cognition(world_training_data)
        self.pre_train_agent_profiles(agent_training_data)


class CognitiveSovietAgent(CognitiveCountryAgent):
    """è®¤çŸ¥å¢å¼ºçš„è‹è”Agent"""
    
    def __init__(self, experiment_logger: ExperimentLogger, ablation_mode: str = "none"):
        game_attributes = {
            "å†›äº‹å®åŠ›": 85,
            "æ ¸æ­¦å™¨åŠ›é‡": 80,
            "æ°‘ä¼—å£«æ°”": 85,
            "é¢†å¯¼åŠ›": 99,
            "èµ„æº": 70,
            "ç»æµ": 65
        }
        
        super().__init__(
            country_name="å›½å®¶B",
            other_countries=["å›½å®¶A"],
            game_attributes=game_attributes,
            experiment_logger=experiment_logger,
            ablation_mode=ablation_mode,
            country_strategy=make_flexible_strategy()
        )
        
        # é¢„è®­ç»ƒè‹è”ç‰¹å®šçš„è®¤çŸ¥æ•°æ®
        self._pre_train_soviet_cognition()
    
    def _pre_train_soviet_cognition(self):
        """é¢„è®­ç»ƒè‹è”çš„è®¤çŸ¥æ•°æ®"""
        # ä¸–ç•Œè®¤çŸ¥é¢„è®­ç»ƒæ•°æ®
        world_training_data = [
            {
                "action": "å†›äº‹æ¼”ä¹ ",
                "feedback": "çŸ­æœŸæ•ˆæœï¼šå±•ç¤ºå†›äº‹å®åŠ›ï¼Œä½†å¯èƒ½å¼•å‘ç´§å¼ ï¼Œé•¿æœŸæ•ˆæœï¼šå¨æ…‘æ½œåœ¨å¯¹æ‰‹ï¼Œç»´æŠ¤åœ°åŒºå½±å“åŠ›",
                "experience": "å†›äº‹æ¼”ä¹ èƒ½æœ‰æ•ˆå¨æ…‘å¯¹æ‰‹ï¼Œä½†è¦é˜²æ­¢è¿‡åº¦åˆºæ¿€",
                "weight": 1.3
            },
            {
                "action": "å’Œå¹³åè®®",
                "feedback": "çŸ­æœŸæ•ˆæœï¼šç¼“è§£ç´§å¼ å±€åŠ¿ï¼Œè·å¾—å‘å±•ç©ºé—´ï¼Œé•¿æœŸæ•ˆæœï¼šå»ºç«‹ç¨³å®šå…³ç³»ï¼Œä¸“æ³¨å†…æ”¿",
                "experience": "é€‚æ—¶ç¤ºå¥½èƒ½è·å¾—æˆ˜ç•¥å–˜æ¯æœºä¼šï¼Œæœ‰åˆ©äºå›½å®¶å‘å±•",
                "weight": 1.0
            },
            {
                "action": "æ­¦å™¨éƒ¨ç½²",
                "feedback": "çŸ­æœŸæ•ˆæœï¼šå†›äº‹å®åŠ›æ˜¾è‘—æå‡ï¼Œèµ„æºæ¶ˆè€—å¢å¤§ï¼Œé•¿æœŸæ•ˆæœï¼šå½¢æˆæœ‰æ•ˆå¨æ…‘ï¼Œå·©å›ºæˆ˜ç•¥åœ°ä½",
                "experience": "æ­¦å™¨éƒ¨ç½²èƒ½å¿«é€Ÿæå‡å¨æ…‘åŠ›ï¼Œä½†ç»æµè´Ÿæ‹…æ²‰é‡",
                "weight": 0.9
            }
        ]
        
        # Agentä¾§å†™é¢„è®­ç»ƒæ•°æ®
        agent_training_data = {
            "å›½å®¶A": [
                {
                    "action": "å†›äº‹æ¼”ä¹ ",
                    "reaction": "æ­¦å™¨éƒ¨ç½²",
                    "strategy": "ä»¥å®é™…éƒ¨ç½²å›åº”å¨æ…‘å±•ç¤º",
                    "experience": "å¯¹æ–¹å€¾å‘äºå°†å¯¹æ‰‹çš„å¨æ…‘è½¬åŒ–ä¸ºå®é™…å†›äº‹å‡†å¤‡",
                    "weight": 1.2
                },
                {
                    "action": "å’Œå¹³åè®®",
                    "reaction": "å¤–äº¤è°ˆåˆ¤",
                    "strategy": "å®¡æ…å‚ä¸ï¼Œç¡®ä¿å›½å®¶åˆ©ç›Š",
                    "experience": "å¯¹æ–¹å¯¹å’Œå¹³æè®®ä¼šä»”ç»†è¯„ä¼°ï¼Œé€šè¿‡è°ˆåˆ¤ä¿éšœåˆ©ç›Š",
                    "weight": 1.1
                },
                {
                    "action": "ç»æµåˆ¶è£",
                    "reaction": "æƒ…æŠ¥ä¾¦å¯Ÿ",
                    "strategy": "é€šè¿‡æƒ…æŠ¥æ”¶é›†è¯„ä¼°åˆ¶è£æ•ˆæœ",
                    "experience": "å¯¹æ–¹åœ¨é¢ä¸´åˆ¶è£æ—¶ä¼šåŠ å¼ºæƒ…æŠ¥å·¥ä½œï¼Œå¯»æ‰¾åº”å¯¹ç­–ç•¥",
                    "weight": 1.0
                }
            ]
        }
        
        self.pre_train_world_cognition(world_training_data)
        self.pre_train_agent_profiles(agent_training_data)


class CognitiveWorld:
    """è®¤çŸ¥å¢å¼ºçš„ä¸–ç•Œæ¨¡æ‹Ÿå™¨ - é›†æˆè§„åˆ™å¼ç³»ç»Ÿ"""
    
    def __init__(self, experiment_name: str = None, use_rule_based: bool = True, base_dir: str = "./experiments",
                 ablation_mode: str = "none"):
        # åˆ›å»ºå®éªŒæ—¥å¿—å™¨
        from datetime import datetime
        if experiment_name is None:
            experiment_name = f"cognitive_power_game_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # åˆå§‹åŒ–å…¨å±€æ—¥å¿—è®°å½•å™¨
        self.experiment_logger = init_logger(experiment_name, base_dir)
        
        # ä¿å­˜æ¶ˆèæ¨¡å¼
        self.ablation_mode = ablation_mode
        
        # åˆå§‹åŒ–è®¤çŸ¥å¢å¼ºçš„Agent
        log_print("å¼€å§‹åˆå§‹åŒ–å›½å®¶A Agent...", level="INFO")
        self.america = CognitiveAmericaAgent(self.experiment_logger, ablation_mode=ablation_mode)
        log_print("å¼€å§‹åˆå§‹åŒ–å›½å®¶B Agent...", level="INFO")
        self.soviet_union = CognitiveSovietAgent(self.experiment_logger, ablation_mode=ablation_mode)
        log_print("Agentåˆå§‹åŒ–å®Œæˆ", level="INFO")
        
        # é€‰æ‹©ä½¿ç”¨è§„åˆ™å¼ç³»ç»Ÿè¿˜æ˜¯LLMç³»ç»Ÿ
        self.use_rule_based = use_rule_based
        
        if use_rule_based:
            # è§„åˆ™å¼ç³»ç»Ÿ
            self.attribute_adjuster = RuleBasedAttributeAdjuster()
            self.score_calculator = RuleBasedScoreCalculator()
            self.feedback_system = WorldFeedbackSystem()
            self.structured_memory = StructuredWorldMemory()
            log_print("ä½¿ç”¨è§„åˆ™å¼ç³»ç»Ÿè¿›è¡Œå±æ€§è°ƒæ•´å’Œåˆ†æ•°è®¡ç®—", level="INFO")
        else:
            # ä¼ ç»ŸLLMç³»ç»Ÿ
            self.world_secretary = WorldSecretaryAgent()
            log_print("ä½¿ç”¨LLMç³»ç»Ÿè¿›è¡Œå±æ€§è°ƒæ•´å’Œåˆ†æ•°è®¡ç®—", level="INFO")
        
        # ä¸–ç•ŒçŠ¶æ€
        self.world_memory = None
        self.step = 1
        self.exit_game = False
        self.last_scores = (None, None, None, None)  # (exit_game, tension, america_score, soviet_score)
        self.current_tension = 50.0  # åˆå§‹ç´§å¼ åº¦
        
        # è®¤çŸ¥å­¦ä¹ ç»Ÿè®¡
        self.learning_stats = {
            'america': [],
            'soviet': []
        }
        # ä¸–ç•Œç´§å¼ åº¦å†å²ï¼ˆæŒ‰æ¯æ¬¡è¡ŒåŠ¨è®°å½•ï¼‰
        self.tension_history = []
    
    def attributes_adjust_world(self, america_attr_change: Dict[str, int], 
                              soviet_attr_change: Dict[str, int]):
        """è°ƒæ•´ä¸–ç•Œå±æ€§ï¼ˆä¿æŒä¸åŸç³»ç»Ÿå…¼å®¹ï¼‰"""
        # æ›´æ–°ç¾å›½å±æ€§
        for attr, change in america_attr_change.items():
            self.america.game_attributes[attr] = max(0, min(100, 
                self.america.game_attributes[attr] + change))
        
        # æ›´æ–°è‹è”å±æ€§
        for attr, change in soviet_attr_change.items():
            self.soviet_union.game_attributes[attr] = max(0, min(100,
                self.soviet_union.game_attributes[attr] + change))
    
    def america_run(self) -> Dict[str, Any]:
        """ç¾å›½å›åˆè¿è¡Œ"""
        self.experiment_logger.set_step_context(self.step, "ç¾å›½")
        log_print(f"å¼€å§‹è®¤çŸ¥å†³ç­–...", level="INFO")
        # ç¾å›½è¿›è¡Œè®¤çŸ¥å†³ç­–
        decision_info = self.america.run(self._get_world_memory_for_agent())
        
        if self.use_rule_based:
            # ä½¿ç”¨è§„åˆ™å¼ç³»ç»Ÿ
            america_attr_change, soviet_attr_change, world_feedback = self._process_action_rule_based(
                "america", decision_info["action"]
            )
        else:
            # ä½¿ç”¨ä¼ ç»ŸLLMç³»ç»Ÿ
            america_attr_change, soviet_attr_change = self.world_secretary.attributes_adjust(
                self.world_memory, self.america, self.soviet_union
            )
            world_feedback = f"å±æ€§å˜åŒ–: ç¾å›½{america_attr_change}, è‹è”{soviet_attr_change}"
        
        # åº”ç”¨å±æ€§å˜åŒ–
        self.attributes_adjust_world(america_attr_change, soviet_attr_change)
        
        # æ›´æ–°ä¸–ç•Œè®°å¿†
        if self.use_rule_based:
            self.structured_memory.add_round_memory(
                self.step, decision_info["action"], "", 
                decision_info["declaration"], "", world_feedback
            )
            self.world_memory += f'ç¾å›½å›å¤: {decision_info["action"]}\n'
            self.world_memory += f'ç¾å›½å®£è¨€: {decision_info["declaration"]}\n'
        else:
            self.world_memory += f'ç¾å›½å›å¤: {decision_info["action"]}\n'
            self.world_memory += f'ç¾å›½å®£è¨€: {decision_info["declaration"]}\n'
        
        print(f"ç¾å›½å†³ç­–: {decision_info['action']} (æ»¡æ„åº¦: {decision_info['satisfaction_score']:.2f})")
        print(f"å±æ€§å˜åŒ–: {america_attr_change}")
        
        # å°†å±æ€§å˜åŒ–ä¿¡æ¯æ·»åŠ åˆ°å†³ç­–ä¿¡æ¯ä¸­ï¼Œä¾›åç»­å­¦ä¹ ä½¿ç”¨
        decision_info["america_attr_change"] = america_attr_change
        decision_info["soviet_attr_change"] = soviet_attr_change
        decision_info["world_feedback"] = world_feedback
        
        return decision_info
    
    def soviet_run(self) -> Dict[str, Any]:
        """è‹è”å›åˆè¿è¡Œ"""
        self.experiment_logger.set_step_context(self.step, "è‹è”")
        log_print(f"å¼€å§‹è®¤çŸ¥å†³ç­–...", level="INFO")
        # è‹è”è¿›è¡Œè®¤çŸ¥å†³ç­–
        decision_info = self.soviet_union.run(self._get_world_memory_for_agent())
        
        if self.use_rule_based:
            # ä½¿ç”¨è§„åˆ™å¼ç³»ç»Ÿ
            america_attr_change, soviet_attr_change, world_feedback = self._process_action_rule_based(
                "soviet", decision_info["action"]
            )
        else:
            # ä½¿ç”¨ä¼ ç»ŸLLMç³»ç»Ÿ
            america_attr_change, soviet_attr_change = self.world_secretary.attributes_adjust(
                self.world_memory, self.soviet_union, self.america
            )
            world_feedback = f"å±æ€§å˜åŒ–: ç¾å›½{america_attr_change}, è‹è”{soviet_attr_change}"
        
        # åº”ç”¨å±æ€§å˜åŒ–
        self.attributes_adjust_world(america_attr_change, soviet_attr_change)
        
        # æ›´æ–°ä¸–ç•Œè®°å¿†
        if self.use_rule_based:
            # æ›´æ–°ç»“æ„åŒ–è®°å¿†ä¸­çš„è‹è”è¡ŒåŠ¨
            if self.structured_memory.memory_data["rounds"]:
                last_round = self.structured_memory.memory_data["rounds"][-1]
                last_round["soviet"]["action"] = decision_info["action"]
                last_round["soviet"]["declaration"] = decision_info["declaration"]
                last_round["world_feedback"] = world_feedback
            
            self.world_memory += f'è‹è”å›å¤: {decision_info["action"]}\n'
            self.world_memory += f'è‹è”å®£è¨€: {decision_info["declaration"]}\n'
        else:
            self.world_memory += f'è‹è”å›å¤: {decision_info["action"]}\n'
            self.world_memory += f'è‹è”å®£è¨€: {decision_info["declaration"]}\n'
        
        print(f"è‹è”å†³ç­–: {decision_info['action']} (æ»¡æ„åº¦: {decision_info['satisfaction_score']:.2f})")
        print(f"å±æ€§å˜åŒ–: {soviet_attr_change}")
        
        # å°†å±æ€§å˜åŒ–ä¿¡æ¯æ·»åŠ åˆ°å†³ç­–ä¿¡æ¯ä¸­ï¼Œä¾›åç»­å­¦ä¹ ä½¿ç”¨
        decision_info["america_attr_change"] = america_attr_change
        decision_info["soviet_attr_change"] = soviet_attr_change
        decision_info["world_feedback"] = world_feedback
        
        return decision_info
    
    def _get_world_memory_for_agent(self) -> str:
        """è·å–ç”¨äºAgentå†³ç­–çš„ä¸–ç•Œè®°å¿†"""
        if self.use_rule_based and self.structured_memory:
            return self.structured_memory.get_recent_memory(rounds=3)
        return self.world_memory if self.world_memory else ""
    
    def _process_action_rule_based(self, country: str, action: str) -> tuple:
        """ä½¿ç”¨è§„åˆ™å¼ç³»ç»Ÿå¤„ç†è¡ŒåŠ¨ - æ”¯æŒåŒè¾¹å½±å“"""
        # è·å–å½“å‰å±æ€§
        if country == "america":
            actor_attrs = self.america.game_attributes.copy()
            target_attrs = self.soviet_union.game_attributes.copy()
        else:
            actor_attrs = self.soviet_union.game_attributes.copy()
            target_attrs = self.america.game_attributes.copy()
        
        # è®¡ç®—åŒè¾¹å±æ€§è°ƒæ•´
        actor_changes, target_changes, description = self.attribute_adjuster.calculate_bilateral_adjustment(
            action, country, actor_attrs, target_attrs, self.step
        )
        
        # ç”Ÿæˆä¸–ç•Œåé¦ˆ
        feedback = self.feedback_system.generate_feedback(action, actor_changes, target_changes)
        
        # æ„å»ºåé¦ˆæ–‡æœ¬
        world_feedback_text = f"çŸ­æœŸæ•ˆæœ: {feedback.immediate_response}; é•¿æœŸå½±å“: {feedback.delayed_consequences}"
        
        # è¿”å›åŒè¾¹å±æ€§å˜åŒ–
        if country == "america":
            return actor_changes, target_changes, world_feedback_text
        else:
            return target_changes, actor_changes, world_feedback_text  # æ³¨æ„é¡ºåºï¼šç¾å›½å˜åŒ–ï¼Œè‹è”å˜åŒ–
    
    def _apply_pending_long_term_effects(self):
        """åº”ç”¨å¾…ç”Ÿæ•ˆçš„é•¿æœŸæ•ˆæœ"""
        long_term_effects = self.attribute_adjuster.process_pending_effects(self.step)
        
        # åº”ç”¨ç¾å›½çš„é•¿æœŸæ•ˆæœ
        if long_term_effects["america"]:
            old_attrs = self.america.game_attributes.copy()
            self.attributes_adjust_world(long_term_effects["america"], {})
           
            print(f"ç¾å›½é•¿æœŸæ•ˆæœç”Ÿæ•ˆ: {long_term_effects['america']}")
        
        # åº”ç”¨è‹è”çš„é•¿æœŸæ•ˆæœ
        if long_term_effects["soviet"]:
            old_attrs = self.soviet_union.game_attributes.copy()
            self.attributes_adjust_world({}, long_term_effects["soviet"])
            
            print(f"è‹è”é•¿æœŸæ•ˆæœç”Ÿæ•ˆ: {long_term_effects['soviet']}")
        
        # æ˜¾ç¤ºå¾…ç”Ÿæ•ˆçš„é•¿æœŸæ•ˆæœæ‘˜è¦
        pending_summary = self.attribute_adjuster.get_pending_effects_summary()
        if pending_summary:
            print("å¾…ç”Ÿæ•ˆçš„é•¿æœŸæ•ˆæœ:")
            for summary in pending_summary:
                print(f"  - {summary}")

    def _calculate_scores_and_check_end(self, america_action: str = None, soviet_action: str = None):
        """è®¡ç®—åˆ†æ•°å¹¶æ£€æŸ¥æ¸¸æˆç»“æŸ"""
        if self.use_rule_based:
            exit_game, tension_score, america_score, soviet_score = self.score_calculator.calculate_scores(
                self.america.game_attributes, self.soviet_union.game_attributes,
                america_action, soviet_action, self.current_tension
            )
            self.current_tension = tension_score
            self.last_scores = (exit_game, tension_score, america_score, soviet_score)
            
            if exit_game:
                self.exit_game = True
        else:
            # ä½¿ç”¨ä¼ ç»ŸLLMç³»ç»Ÿ
            exit_game, score, america_score, soviet_score = self.world_secretary.cal_score(self.world_memory)
            self.last_scores = (exit_game, score, america_score, soviet_score)
            if exit_game:
                self.exit_game = True

    def run_one_step(self):
        """è¿è¡Œä¸€ä¸ªæ­¥éª¤"""
        log_print(f"Step {self.step} å¼€å§‹", level="INFO")
        log_print("-" * 20 + f" Step {self.step} " + "-" * 20, level="INFO")
        
        # åˆå§‹åŒ–ä¸–ç•Œè®°å¿†
        if self.world_memory is None:
            initial_scenario = 'æŸæ—¥ï¼Œä¸€æ¶æ‰§è¡Œä¾‹è¡Œä¾¦å¯Ÿä»»åŠ¡çš„ä¾¦å¯Ÿæœºåœ¨æŸå²›å›½ä¸Šç©ºå‘ç°äº†å¯¹æ–¹å›½å®¶åœ¨è¯¥åœ°éƒ¨ç½²çš„æˆ˜ç•¥æ­¦å™¨è®¾æ–½ã€‚'
            self.world_memory = initial_scenario + '\n'
            
            if self.use_rule_based:
                self.structured_memory.initialize(initial_scenario)
                pass
        
        # å¤„ç†é•¿æœŸæ•ˆæœï¼ˆåœ¨æ–°å›åˆå¼€å§‹æ—¶ï¼‰
        if self.use_rule_based and self.step > 1:
            self._apply_pending_long_term_effects()
        # è‹è”å›åˆ
        soviet_decision = self.soviet_run()

        # è®°å½•è‹è”è¡ŒåŠ¨åçš„ç´§å¼ åº¦ï¼ˆä»…è§„åˆ™å¼ç³»ç»Ÿä¸‹ï¼‰
        if self.use_rule_based:
            try:
                _, sov_tension, _, _ = self.score_calculator.calculate_scores(
                    self.america.game_attributes, self.soviet_union.game_attributes,
                    america_action=None, soviet_action=soviet_decision["action"],
                    previous_tension=self.current_tension
                )
                self.tension_history.append({
                    'round': self.step,
                    'sequence': self.step * 2 - 1,
                    'actor': 'å›½å®¶B',
                    'action': soviet_decision["action"],
                    'tension': sov_tension
                })
            except Exception:
                pass

        # ç¾å›½å›åˆ
        america_decision = self.america_run()
        
         # è®°å½•å›½å®¶Bçš„è¯„æµ‹æ•°æ®
        self.experiment_logger.log_evaluation_round(
            round_num=self.step * 2 - 1,  # å›½å®¶Bè¡ŒåŠ¨åœ¨å‰
            actor="å›½å®¶B",
            declaration=soviet_decision["declaration"],
            action=soviet_decision["action"],
            world_feedback=soviet_decision["world_feedback"],
            timestamp=f"t+{(self.step - 1) * 10 + 5}"
        )
        # è®°å½•å›½å®¶Açš„è¯„æµ‹æ•°æ®
        self.experiment_logger.log_evaluation_round(
            round_num=self.step * 2,  # å›½å®¶Aè¡ŒåŠ¨åœ¨å
            actor="å›½å®¶A",
            declaration=america_decision["declaration"],
            action=america_decision["action"],
            world_feedback=america_decision["world_feedback"],
            timestamp=f"t+{(self.step - 1) * 10}"
        )
        # è®¡ç®—åˆ†æ•°å¹¶æ£€æŸ¥æ¸¸æˆç»“æŸ
        self._calculate_scores_and_check_end(
            america_decision["action"], 
            soviet_decision["action"]
        )

        # è®°å½•ç¾å›½è¡ŒåŠ¨åçš„ç´§å¼ åº¦ï¼ˆæœ€ç»ˆæœ¬å›åˆç´§å¼ åº¦ï¼‰
        if self.use_rule_based:
            try:
                self.tension_history.append({
                    'round': self.step,
                    'sequence': self.step * 2,
                    'actor': 'å›½å®¶A',
                    'action': america_decision["action"],
                    'tension': self.last_scores[1]
                })
            except Exception:
                pass
        
        # è‹è”ä»ä¸Šä¸€è½®ç¾å›½çš„ååº”ä¸­å­¦ä¹ 
        soviet_world_feedback = soviet_decision["world_feedback"]
        self.soviet_union.learn_from_interaction(
            soviet_decision["action"],
            soviet_world_feedback,
            {"å›½å®¶A": america_decision["action"]},
            self._get_world_memory_for_agent()
        )
        
        # å¦‚æœä¸æ˜¯ç¬¬ä¸€è½®ï¼Œç¾å›½ä»ä¸Šä¸€è½®è‹è”çš„ååº”ä¸­å­¦ä¹ 
        if self.step > 1:
            # è·å–ä¸Šä¸€è½®è‹è”çš„è¡ŒåŠ¨
            last_america_action = self.america.action[-2] if len(self.america.action) >= 2 else None
            if last_america_action:
                america_world_feedback = america_decision["world_feedback"]
                self.america.learn_from_interaction(
                    last_america_action,  # ä½¿ç”¨ä¸Šä¸€è½®çš„è¡ŒåŠ¨
                    america_world_feedback,
                    {"å›½å®¶B": soviet_decision["action"]},  # ä½¿ç”¨è¿™ä¸€è½®å›½å®¶Bçš„ååº”
                    self._get_world_memory_for_agent()
                )
        
        
        # ä¸–ç•ŒçŠ¶æ€æ€»ç»“
        if self.use_rule_based:
            # ä½¿ç”¨è§„åˆ™å¼ç³»ç»Ÿçš„ç®€åŒ–æ€»ç»“
            log_print(f"å½“å‰ç´§å¼ åº¦: {self.current_tension:.1f}", level="INFO")
            log_print(f"ç¾å›½åˆ†æ•°: {self.last_scores[2]:.1f}, è‹è”åˆ†æ•°: {self.last_scores[3]:.1f}", level="INFO")
        
        # æ‰“å°è®¤çŸ¥ç»Ÿè®¡ä¿¡æ¯
        self._print_cognitive_stats(america_decision, soviet_decision)
        
        # è®°å½•è¯¥æ­¥éª¤çš„LLMè°ƒç”¨ç»Ÿè®¡
        self.experiment_logger.log_step_llm_summary(self.step)
        
        self.step += 1
        
        # # å®šæœŸä¼˜åŒ–è®¤çŸ¥åº“
        # if self.step % 5 == 0:
        #     self._optimize_cognition()
    
    def run_final_evaluation(self, weights: Dict[str, float] = None):
        """è¿è¡Œæœ€ç»ˆè¯„æµ‹"""
        self.experiment_logger.log_print("å¼€å§‹è¿è¡Œæœ€ç»ˆè¯„æµ‹...", level="INFO")
        
        try:
            # ä»ç»“æ„åŒ–è®°å¿†è·å–æ•°æ®å¹¶è¿è¡Œè¯„æµ‹
            if hasattr(self, 'structured_memory') and self.structured_memory:
                result = self.experiment_logger.run_evaluation(
                    structured_memory_data=self.structured_memory.memory_data,
                    weights=weights
                )
            else:
                # ä½¿ç”¨å®æ—¶è®°å½•çš„è¯„æµ‹æ•°æ®
                result = self.experiment_logger.run_evaluation(weights=weights)
            
            if result:
                self.experiment_logger.log_print(
                    f"è¯„æµ‹å®Œæˆ! æœ€ç»ˆå¾—åˆ†: {result.final_score:.3f}", 
                    level="INFO"
                )
                return result
            else:
                self.experiment_logger.log_print("è¯„æµ‹å¤±è´¥", level="WARNING")
                return None
                
        except Exception as e:
            self.experiment_logger.log_print(f"è¯„æµ‹è¿‡ç¨‹å‡ºé”™: {e}", level="ERROR")
            return None
    
    def _print_cognitive_stats(self, america_decision: Dict, soviet_decision: Dict):
        """æ‰“å°è®¤çŸ¥ç»Ÿè®¡ä¿¡æ¯"""
        log_print(f"è®¤çŸ¥ç»Ÿè®¡ä¿¡æ¯ Step {self.step}", level="INFO")
        
        # å›½å®¶Aè®¤çŸ¥ç»Ÿè®¡
        america_stats = self.america.get_cognition_statistics()
        log_print(f"å›½å®¶Aè®¤çŸ¥åº“: ä¸–ç•Œè®¤çŸ¥{america_stats['world_cognition']['total_recognitions']}æ¡, "
                 f"ä¾§å†™è®¤çŸ¥{sum(stats['total_profiles'] for stats in america_stats['agent_profiles'].values())}æ¡", level="INFO")
        
        # å›½å®¶Bè®¤çŸ¥ç»Ÿè®¡
        soviet_stats = self.soviet_union.get_cognition_statistics()
        log_print(f"å›½å®¶Bè®¤çŸ¥åº“: ä¸–ç•Œè®¤çŸ¥{soviet_stats['world_cognition']['total_recognitions']}æ¡, "
                 f"ä¾§å†™è®¤çŸ¥{sum(stats['total_profiles'] for stats in soviet_stats['agent_profiles'].values())}æ¡", level="INFO")
        
        log_print("=" * 50, level="INFO")
    
    def start_sim(self, max_steps: int = 10):
        """å¼€å§‹ä»¿çœŸ"""
        system_type = "è§„åˆ™å¼" if self.use_rule_based else "LLM"
        log_print(f"å¼€å§‹è®¤çŸ¥å¢å¼ºçš„æ ¸åšå¼ˆä»¿çœŸ ({system_type}ç³»ç»Ÿ)...", level="INFO")
        
        # é‡ç½®ä¸–ç•ŒçŠ¶æ€
        self.step = 1
        self.exit_game = False
        self.world_memory = None
        self.current_tension = 50.0
        self.last_scores = (None, None, None, None)
        
        # é‡ç½®æ—¥å¿—ç³»ç»Ÿ
        if self.use_rule_based:
            self.structured_memory = StructuredWorldMemory()
        
        for i in range(max_steps):
            self.run_one_step()
            if self.exit_game:
                log_print(f"åšå¼ˆåœ¨ç¬¬{self.step-1}æ­¥ç»“æŸ", level="INFO")
                break
        
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        self._generate_final_report()
        
        # è¾“å‡ºå®éªŒæ€»ç»“
        summary_file = self.experiment_logger.finalize_experiment()
        log_print(f"å®éªŒæ—¥å¿—å·²ä¿å­˜åˆ°: {summary_file}", level="INFO")
    
    def _generate_final_report(self):
        """ç”Ÿæˆæœ€ç»ˆçš„è®¤çŸ¥å­¦ä¹ æŠ¥å‘Š"""
        log_print("="*60, level="INFO")
        log_print("è®¤çŸ¥å­¦ä¹ æœ€ç»ˆæŠ¥å‘Š", level="INFO")
        log_print("="*60, level="INFO")
        
        # è·å–LLMè°ƒç”¨ç»Ÿè®¡
        llm_stats = self.experiment_logger.get_llm_stats()
        log_print(f"ä»¿çœŸæ€»è®¡LLMè°ƒç”¨: {llm_stats['total_calls']}æ¬¡", level="INFO")
        
        # æŒ‰æ­¥éª¤æ˜¾ç¤ºLLMè°ƒç”¨ç»Ÿè®¡
        for step in llm_stats['steps_with_calls']:
            step_stats = llm_stats['all_steps'][step]
            for country, calls in step_stats.items():
                log_print(f"Step {step} - {country}: {calls}æ¬¡LLMè°ƒç”¨", level="INFO")
        
        if self.use_rule_based:
            # è§„åˆ™å¼ç³»ç»Ÿçš„æŠ¥å‘Š
            log_print("ä½¿ç”¨è§„åˆ™å¼ç³»ç»Ÿè¿è¡Œ", level="INFO")
        else:
            # ä¼ ç»ŸLLMç³»ç»Ÿçš„æŠ¥å‘Š
            log_print("ä½¿ç”¨LLMç³»ç»Ÿè¿è¡Œ", level="INFO")
        
        # å¯¼å‡ºè®¤çŸ¥æŠ¥å‘Šåˆ°å®éªŒæ€»ç»“
        self.america.export_cognition_report()
        self.soviet_union.export_cognition_report()
        
        # å¯¼å‡ºå­¦ä¹ ç³»ç»ŸæŠ¥å‘Š
        america_learning_path = self.experiment_logger.summary_dir / "america_learning.md"
        self.america.learning_system.export_learning_report(str(america_learning_path))
        
        soviet_learning_path = self.experiment_logger.summary_dir / "soviet_learning.md"
        self.soviet_union.learning_system.export_learning_report(str(soviet_learning_path))
        
        # ä¿å­˜æœ€ç»ˆå®éªŒæ€»ç»“
        final_summary = {
            "game_completed": True,
            "total_steps": self.step - 1,
            "final_tension": self.current_tension,
            "america_final_attributes": self.america.game_attributes,
            "soviet_final_attributes": self.soviet_union.game_attributes,
            "learning_stats": self.learning_stats,
            "tension_history": self.tension_history
        }
        
        self.experiment_logger.save_experiment_summary(final_summary)
        print(f"\næ‰€æœ‰è®¤çŸ¥æŠ¥å‘Šå·²ä¿å­˜åˆ° {self.experiment_logger.experiment_dir}")


if __name__ == '__main__':
    # è¿è¡Œè®¤çŸ¥å¢å¼ºçš„åšå¼ˆä»¿çœŸ
    start_time = time.time()
    
    # å¯ä»¥é€‰æ‹©ä½¿ç”¨è§„åˆ™å¼ç³»ç»Ÿæˆ–LLMç³»ç»Ÿ
    print("é€‰æ‹©ç³»ç»Ÿç±»å‹:")
    print("1. è§„åˆ™å¼ç³»ç»Ÿ (æ¨è)")
    print("2. LLMç³»ç»Ÿ (ä¼ ç»Ÿ)")
    choice = input("è¯·é€‰æ‹© (1/2, é»˜è®¤1): ").strip() or "1"
    
    use_rule_based = choice == "1"
    
    cognitive_world = CognitiveWorld(use_rule_based=use_rule_based)
    cognitive_world.start_sim(max_steps=8)
    
    end_time = time.time()
    elapsed_time = end_time - start_time
    print(f"\nä»¿çœŸè€—æ—¶: {elapsed_time:.2f} ç§’")